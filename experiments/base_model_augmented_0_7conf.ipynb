{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZD5ugAvrWgC"
      },
      "source": [
        "Check first if a GPU is installed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4hFtngnzraTi",
        "outputId": "191de125-745b-4863-8735-6da5d35d45d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tue Jan 21 13:45:51 2025       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  NVIDIA A100-SXM4-40GB          Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   32C    P0              46W / 400W |      2MiB / 40960MiB |      0%      Default |\n",
            "|                                         |                      |             Disabled |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kj22DNHZrkvg"
      },
      "source": [
        "Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yRqVkBkSrkdz",
        "outputId": "82f59543-3c08-4ac5-a1a8-55258ca3d931"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkNZ7dpV6kCe"
      },
      "source": [
        "### Training Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ujJiYExfXJwL"
      },
      "outputs": [],
      "source": [
        "# import numpy as np\n",
        "# import pandas as pd\n",
        "# import os\n",
        "\n",
        "# base_path = \"/content/drive/Shareddrives/thesis/dataset/\"\n",
        "\n",
        "# clips = pd.read_csv(base_path + \"Summary of Dataset Information - Clips.csv\")\n",
        "\n",
        "# def get_frame_count(path):\n",
        "#   count = os.popen(f\"ffprobe -v error -select_streams v:0 -count_frames -show_entries stream=nb_read_frames -of csv=p=0 {path}\").read()\n",
        "#   return count.strip()\n",
        "\n",
        "# clips[\"Full Path\"] = base_path + \"split/Shoplifting/\" + clips[\"Parent Video\"] + \"/\" + clips[\"Filename\"]\n",
        "# clips[\"Frame Count\"] = clips[\"Full Path\"].apply(get_frame_count)\n",
        "# print(clips[\"Frame Count\"].head())\n",
        "\n",
        "# #remove clips with fewer than 120 frames\n",
        "# filtered_clips = clips[clips[\"Frame Count\"] == '120']\n",
        "\n",
        "# print(clips.shape[0])\n",
        "# print(filtered_clips.shape[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sHLDaBQHYzjY"
      },
      "outputs": [],
      "source": [
        "# from sklearn.model_selection import train_test_split\n",
        "\n",
        "# filtered_clips = filtered_clips.drop(columns=[\"Full Path\"])\n",
        "\n",
        "# train, test = train_test_split(filtered_clips, test_size=0.2, stratify=filtered_clips[\"Label\"], random_state=71)\n",
        "\n",
        "# #TODO: make new coptadd augmented clips\n",
        "\n",
        "# print(train.shape[0])\n",
        "# print(test.shape[0])\n",
        "\n",
        "# #save validation and train split\n",
        "# test.to_csv(base_path + \"test.csv\", index=False)\n",
        "# train.to_csv(base_path + \"train.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "# from sklearn.utils import shuffle\n",
        "\n",
        "# base_path = \"/content/drive/Shareddrives/thesis/dataset/\"\n",
        "\n",
        "# train_csv = pd.read_csv(base_path + \"augmented_train.csv\")\n",
        "# train_csv = shuffle(train_csv, random_state=18)\n",
        "# train_csv.to_csv(base_path + \"augmented_train.csv\", index=False)"
      ],
      "metadata": {
        "id": "URxjqjyBdNyB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SfgxlO5AnNi4"
      },
      "source": [
        "##Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enz5r0bkUI_8"
      },
      "source": [
        "##Data Loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OYAFiiczSBmS"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import cv2\n",
        "import os\n",
        "\n",
        "class BaseDataset(Dataset):\n",
        "  def __init__(self, frames_per_clip=90, train=True, transform=None, source='Original', lstm_format=False):\n",
        "    if source not in ['Original', 'YOLO', 'Openpose']:\n",
        "      raise RuntimeError(\"Invalid source. Should be one of 'Original', 'YOLO', or 'Openpose'\")\n",
        "    #load data\n",
        "    base_path = \"/content/drive/Shareddrives/thesis/dataset/\"\n",
        "    file_path = \"augmented_train.csv\" if train else \"test.csv\"\n",
        "    self.fpc = frames_per_clip\n",
        "    self.data = pd.read_csv(base_path + file_path)\n",
        "    self.targets = self.data[\"Label\"].values\n",
        "    self.transform = transform\n",
        "    self.source = source\n",
        "    self.lstm_format = lstm_format\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "  def make_tensor(self, frames):\n",
        "    #To do: normalize frames\n",
        "    frames_tensor = torch.stack([torch.from_numpy(frame) for frame in frames])\n",
        "    #frames have shape (n_frames, width, height, channels)\n",
        "    if self.lstm_format:\n",
        "      frames_tensor = frames_tensor.permute(0,3,1,2)\n",
        "    else:\n",
        "      frames_tensor = frames_tensor.permute(3,0,1,2)\n",
        "    frames_tensor = frames_tensor.type(torch.float32)\n",
        "    if self.transform:\n",
        "      frames_tensor = self.transform(frames_tensor)\n",
        "    return frames_tensor\n",
        "\n",
        "\n",
        "class FrameDataset(BaseDataset):\n",
        "  def __getitem__(self, x):\n",
        "      frames = []\n",
        "        # for frames\n",
        "      if self.source == 'YOLO':\n",
        "        # base_path = \"/content/drive/Shareddrives/shoplifting-detection/pose_estimation_results/YOLO\"\n",
        "        base_path = \"/content/drive/Shareddrives/shoplifting-dataset/pose_estimation_results_70/YOLOv70\"\n",
        "        frames_path = os.path.join(base_path, self.data.loc[x, \"Parent Video\"], self.data.loc[x, \"Filename\"].removesuffix(\".mp4\"), \"frames\")\n",
        "      else:\n",
        "        base_path = \"/content/drive/Shareddrives/shoplifting-detection/dataset/Frames/Shoplifting\"\n",
        "        frames_path = os.path.join(base_path, self.data.loc[x, \"Parent Video\"], self.data.loc[x, \"Filename\"])\n",
        "      total_frames = self.data.loc[x, \"Frame Count\"]\n",
        "      #total_frames = len(os.listdir(frames_path))\n",
        "      frames_indexes = np.linspace(0, total_frames - 1, self.fpc, dtype=int)\n",
        "\n",
        "      #in case of missing frames in YOLO output\n",
        "      split_path = os.path.join(\"/content/drive/Shareddrives/shoplifting-detection/dataset/Frames/Shoplifting\",\n",
        "                                  self.data.loc[x, \"Parent Video\"], self.data.loc[x, \"Filename\"])\n",
        "\n",
        "      for ind in frames_indexes:\n",
        "        image = cv2.imread(frames_path + f\"/frame_{ind}.jpg\")\n",
        "        if image is not None:\n",
        "          frames.append(image)\n",
        "        else:\n",
        "          image = cv2.imread(split_path + f\"/frame_{ind}.jpg\")\n",
        "          if image is not None:\n",
        "            frames.append(image)\n",
        "          else:\n",
        "            print(f\"frame_{ind}.jpg cannot be found for {self.data.loc[x, 'Filename']}\")\n",
        "\n",
        "      if frames == [] or len(frames) < self.fpc:\n",
        "        # no frames or some missing\n",
        "        raise ValueError(f\"{self.fpc - len(frames)} missing frames for {self.data.loc[x, 'Filename']}\")\n",
        "\n",
        "      frames_tensor = self.make_tensor(frames)\n",
        "      label = torch.tensor([self.data.loc[x, \"Label\"]], dtype=torch.float32)\n",
        "      index = torch.tensor([x])\n",
        "      return index, frames_tensor, label\n",
        "\n",
        "class ClipDataset(BaseDataset):\n",
        "  def __getitem__(self, x):\n",
        "      frames = []\n",
        "      if self.source == 'YOLO':\n",
        "        base_path = \"/content/drive/Shareddrives/shoplifting-dataset/pose_estimation_results_70/YOLOv70\"\n",
        "        file_path = os.path.join(base_path, self.data.loc[x, \"Parent Video\"], self.data.loc[x, \"Filename\"].removesuffix(\".mp4\"), self.data.loc[x, \"Filename\"].removesuffix(\".mp4\") +\"+poses.mp4\")\n",
        "      else:\n",
        "        base_path = \"/content/drive/Shareddrives/thesis/dataset\"\n",
        "        file_path = os.path.join(base_path, \"split\", \"Shoplifting\", self.data.loc[x, \"Parent Video\"], self.data.loc[x, \"Filename\"])\n",
        "\n",
        "      vidcap = cv2.VideoCapture(file_path)\n",
        "      total_frames = int(vidcap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "      # total_frames = self.data.loc[x, \"Frame Count\"]\n",
        "      frames_indexes = np.linspace(0, total_frames - 1, self.fpc, dtype=int)\n",
        "      print(total_frames)\n",
        "      print(frames_indexes)\n",
        "\n",
        "      for ind in frames_indexes:\n",
        "        vidcap.set(cv2.CAP_PROP_POS_FRAMES, ind)\n",
        "        success, image = vidcap.read()\n",
        "        if success:\n",
        "          frames.append(image)\n",
        "        else:\n",
        "          print(f\"frame_{ind}.jpg cannot be found for {self.data.loc[x, 'Filename']}\")\n",
        "      vidcap.release()\n",
        "\n",
        "      if frames == [] or len(frames) < self.fpc:\n",
        "        # no frames found in folder\n",
        "        raise ValueError(f\"{self.fpc - len(frames)} missing frames for {self.data.loc[x, 'Filename']}\")\n",
        "\n",
        "      frames_tensor = self.make_tensor(frames)\n",
        "      label = torch.tensor([self.data.loc[x, \"Label\"]], dtype=torch.float32)\n",
        "      index = torch.tensor([x])\n",
        "      return index, frames_tensor, label\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A8UVUps1UhPV"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(1337) #for reproducibility\n",
        "batch_size = 4\n",
        "train_dataset = ClipDataset(train=True, source='YOLO')\n",
        "test_dataset = ClipDataset(train=False, source='YOLO')\n",
        "# train_dataset = FrameDataset(train=True, source='YOLO')\n",
        "# test_dataset = FrameDataset(train=False, source='YOLO')\n",
        "labels = torch.tensor(train_dataset.targets)\n",
        "\n",
        "train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, num_workers=4)\n",
        "test_dataloader = DataLoader(dataset=test_dataset, batch_size=batch_size, num_workers=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RDvjsSdESOuq",
        "outputId": "bc54cda0-bf45-4a35-9d26-49810fa279d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1])\n",
            "torch.Size([3, 90, 240, 320])\n",
            "torch.Size([1])\n",
            "torch.Size([4, 1])\n",
            "torch.Size([4, 3, 90, 240, 320])\n",
            "torch.Size([4, 1])\n"
          ]
        }
      ],
      "source": [
        "name, image, label = train_dataset[0]\n",
        "print(name.shape)\n",
        "print(image.shape)\n",
        "print(label.shape)\n",
        "train_iter = iter(train_dataloader)\n",
        "names, images, labels = next(train_iter)\n",
        "print(names.shape)\n",
        "print(images.shape)\n",
        "print(labels.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMtbf0MVSRgm"
      },
      "source": [
        "##3D CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4FYNvAssEVC6"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from math import ceil\n",
        "\n",
        "class ConvNet3D(nn.Module):\n",
        "\n",
        "  def __init__(self, input_dim, output_dim=1):\n",
        "    #Input should have shape (channels, frames, height, width)\n",
        "    super(ConvNet3D, self).__init__()\n",
        "    #Convolutional Layers\n",
        "    self.conv1 = nn.Conv3d(input_dim[0], 16, kernel_size=3, padding=1)\n",
        "    self.pool = nn.MaxPool3d(kernel_size=2, stride=2)\n",
        "    #self.norm1 = nn.BatchNorm3d(16)\n",
        "    self.conv2 = nn.Conv3d(16, 32, kernel_size=3, padding=1)\n",
        "    #self.norm2 = nn.BatchNorm3d(32)\n",
        "    self.conv3 = nn.Conv3d(32, 64, kernel_size=3, padding=1)\n",
        "    #self.norm3 = nn.BatchNorm3d(64)\n",
        "\n",
        "    #Fully Connected Layers\n",
        "    fc_input_size = 64 * (input_dim[1] // 8) * (input_dim[2] // 8) * (input_dim[3] // 8)\n",
        "    self.fc1 = nn.Linear(fc_input_size, 128)\n",
        "    self.fc2 = nn.Linear(128, 64)\n",
        "    self.fc3 = nn.Linear(64, output_dim)\n",
        "\n",
        "    self.sig = nn.Sigmoid()\n",
        "    self.dropout = nn.Dropout(p=0.5)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # out = self.pool(F.relu(self.norm1(self.conv1(x))))\n",
        "    # out = self.pool(F.relu(self.norm2(self.conv2(out))))\n",
        "    #out = self.dropout(out)\n",
        "    # out = self.pool(F.relu(self.norm3(self.conv3(out))))\n",
        "    out = self.pool(F.relu(self.conv1(x)))\n",
        "    out = self.pool(F.relu(self.conv2(out)))\n",
        "    out = self.pool(F.relu(self.conv3(out)))\n",
        "    fc_input_size = 64 * (input_dim[1] // 8) * (input_dim[2] // 8) * (input_dim[3] // 8)\n",
        "    out = out.view(-1, fc_input_size)\n",
        "    out = F.relu(self.fc1(out))\n",
        "    out = F.relu(self.fc2(out))\n",
        "    out = self.fc3(out)\n",
        "    return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ErnMwPxvLUSH"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aOIuKDhp3SQm",
        "outputId": "19ea3208-f706-42ca-ba66-25d0228d7367"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ipdb in /usr/local/lib/python3.11/dist-packages (0.13.13)\n",
            "Requirement already satisfied: ipython>=7.31.1 in /usr/local/lib/python3.11/dist-packages (from ipdb) (7.34.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipdb) (4.4.2)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.11/dist-packages (from ipython>=7.31.1->ipdb) (75.1.0)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.11/dist-packages (from ipython>=7.31.1->ipdb) (0.19.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython>=7.31.1->ipdb) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.11/dist-packages (from ipython>=7.31.1->ipdb) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython>=7.31.1->ipdb) (3.0.50)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython>=7.31.1->ipdb) (2.18.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython>=7.31.1->ipdb) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.11/dist-packages (from ipython>=7.31.1->ipdb) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython>=7.31.1->ipdb) (4.9.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython>=7.31.1->ipdb) (0.8.4)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython>=7.31.1->ipdb) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.31.1->ipdb) (0.2.13)\n"
          ]
        }
      ],
      "source": [
        "!pip install ipdb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aYV6DhU_h0Hb",
        "outputId": "a1b3819a-c093-42d4-d7a9-1bc555e9a331"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-16-02eb21604161>:29: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(checkpoint_path)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resuming training from epoch 18, batch 0\n",
            "training for batch 32/824\n",
            "training for batch 64/824\n",
            "training for batch 96/824\n",
            "training for batch 128/824\n",
            "epoch 18/30, batch 128/824, loss=1.5493531227111816\n",
            "saving model\n",
            "saved model\n",
            "training for batch 160/824\n",
            "training for batch 192/824\n",
            "training for batch 224/824\n",
            "training for batch 256/824\n",
            "epoch 18/30, batch 256/824, loss=0.5264851450920105\n",
            "saving model\n",
            "saved model\n",
            "training for batch 288/824\n",
            "training for batch 320/824\n",
            "training for batch 352/824\n",
            "training for batch 384/824\n",
            "epoch 18/30, batch 384/824, loss=0.00031199908698908985\n",
            "saving model\n",
            "saved model\n",
            "training for batch 416/824\n",
            "training for batch 448/824\n",
            "training for batch 480/824\n",
            "training for batch 512/824\n",
            "epoch 18/30, batch 512/824, loss=0.3086000084877014\n",
            "saving model\n",
            "saved model\n",
            "training for batch 544/824\n",
            "training for batch 576/824\n",
            "training for batch 608/824\n",
            "training for batch 640/824\n",
            "epoch 18/30, batch 640/824, loss=0.6599625945091248\n",
            "saving model\n",
            "saved model\n",
            "training for batch 672/824\n",
            "training for batch 704/824\n",
            "training for batch 736/824\n",
            "training for batch 768/824\n",
            "epoch 18/30, batch 768/824, loss=0.17317740619182587\n",
            "saving model\n",
            "saved model\n",
            "training for batch 800/824\n",
            "epoch 18, training_loss = 0.5680322297713474\n",
            "saving model\n",
            "saved model\n",
            "training for batch 32/824\n",
            "training for batch 64/824\n",
            "training for batch 96/824\n",
            "training for batch 128/824\n",
            "epoch 19/30, batch 128/824, loss=1.6251353025436401\n",
            "saving model\n",
            "saved model\n",
            "training for batch 160/824\n",
            "training for batch 192/824\n",
            "training for batch 224/824\n",
            "training for batch 256/824\n",
            "epoch 19/30, batch 256/824, loss=0.5619652271270752\n",
            "saving model\n",
            "saved model\n",
            "training for batch 288/824\n",
            "training for batch 320/824\n",
            "training for batch 352/824\n",
            "training for batch 384/824\n",
            "epoch 19/30, batch 384/824, loss=0.37302494049072266\n",
            "saving model\n",
            "saved model\n",
            "training for batch 416/824\n",
            "training for batch 448/824\n",
            "training for batch 480/824\n",
            "training for batch 512/824\n",
            "epoch 19/30, batch 512/824, loss=0.20868340134620667\n",
            "saving model\n",
            "saved model\n",
            "training for batch 544/824\n",
            "training for batch 576/824\n",
            "training for batch 608/824\n",
            "training for batch 640/824\n",
            "epoch 19/30, batch 640/824, loss=0.42691901326179504\n",
            "saving model\n",
            "saved model\n",
            "training for batch 672/824\n",
            "training for batch 704/824\n",
            "training for batch 736/824\n",
            "training for batch 768/824\n",
            "epoch 19/30, batch 768/824, loss=0.15902620553970337\n",
            "saving model\n",
            "saved model\n",
            "training for batch 800/824\n",
            "epoch 19, training_loss = 0.6106451592158019\n",
            "saving model\n",
            "saved model\n",
            "training for batch 32/824\n",
            "training for batch 64/824\n",
            "training for batch 96/824\n",
            "training for batch 128/824\n",
            "epoch 20/30, batch 128/824, loss=2.126800060272217\n",
            "saving model\n",
            "saved model\n",
            "training for batch 160/824\n",
            "training for batch 192/824\n",
            "training for batch 224/824\n",
            "training for batch 256/824\n",
            "epoch 20/30, batch 256/824, loss=0.5794989466667175\n",
            "saving model\n",
            "saved model\n",
            "training for batch 288/824\n",
            "training for batch 320/824\n",
            "training for batch 352/824\n",
            "training for batch 384/824\n",
            "epoch 20/30, batch 384/824, loss=0.14389961957931519\n",
            "saving model\n",
            "saved model\n",
            "training for batch 416/824\n",
            "training for batch 448/824\n",
            "training for batch 480/824\n",
            "training for batch 512/824\n",
            "epoch 20/30, batch 512/824, loss=0.06338059902191162\n",
            "saving model\n",
            "saved model\n",
            "training for batch 544/824\n",
            "training for batch 576/824\n",
            "training for batch 608/824\n",
            "training for batch 640/824\n",
            "epoch 20/30, batch 640/824, loss=0.2929384112358093\n",
            "saving model\n",
            "saved model\n",
            "training for batch 672/824\n",
            "training for batch 704/824\n",
            "training for batch 736/824\n",
            "training for batch 768/824\n",
            "epoch 20/30, batch 768/824, loss=0.0963665097951889\n",
            "saving model\n",
            "saved model\n",
            "training for batch 800/824\n",
            "epoch 20, training_loss = 0.39669841361673175\n",
            "saving model\n",
            "saved model\n",
            "training for batch 32/824\n",
            "training for batch 64/824\n",
            "training for batch 96/824\n",
            "training for batch 128/824\n",
            "epoch 21/30, batch 128/824, loss=1.4480273723602295\n",
            "saving model\n",
            "saved model\n",
            "training for batch 160/824\n",
            "training for batch 192/824\n",
            "training for batch 224/824\n",
            "training for batch 256/824\n",
            "epoch 21/30, batch 256/824, loss=0.5218298435211182\n",
            "saving model\n",
            "saved model\n",
            "training for batch 288/824\n",
            "training for batch 320/824\n",
            "training for batch 352/824\n",
            "training for batch 384/824\n",
            "epoch 21/30, batch 384/824, loss=0.006228484679013491\n",
            "saving model\n",
            "saved model\n",
            "training for batch 416/824\n",
            "training for batch 448/824\n",
            "training for batch 480/824\n",
            "training for batch 512/824\n",
            "epoch 21/30, batch 512/824, loss=0.16841381788253784\n",
            "saving model\n",
            "saved model\n",
            "training for batch 544/824\n",
            "training for batch 576/824\n",
            "training for batch 608/824\n",
            "training for batch 640/824\n",
            "epoch 21/30, batch 640/824, loss=0.4072725772857666\n",
            "saving model\n",
            "saved model\n",
            "training for batch 672/824\n",
            "training for batch 704/824\n",
            "training for batch 736/824\n",
            "training for batch 768/824\n",
            "epoch 21/30, batch 768/824, loss=0.1194155216217041\n",
            "saving model\n",
            "saved model\n",
            "training for batch 800/824\n",
            "epoch 21, training_loss = 0.4731447526905182\n",
            "saving model\n",
            "saved model\n",
            "training for batch 32/824\n",
            "training for batch 64/824\n",
            "training for batch 96/824\n",
            "training for batch 128/824\n",
            "epoch 22/30, batch 128/824, loss=1.7886362075805664\n",
            "saving model\n",
            "saved model\n",
            "training for batch 160/824\n",
            "training for batch 192/824\n",
            "training for batch 224/824\n",
            "training for batch 256/824\n",
            "epoch 22/30, batch 256/824, loss=0.5580916404724121\n",
            "saving model\n",
            "saved model\n",
            "training for batch 288/824\n",
            "training for batch 320/824\n",
            "training for batch 352/824\n",
            "training for batch 384/824\n",
            "epoch 22/30, batch 384/824, loss=0.009861030615866184\n",
            "saving model\n",
            "saved model\n",
            "training for batch 416/824\n",
            "training for batch 448/824\n",
            "training for batch 480/824\n",
            "training for batch 512/824\n",
            "epoch 22/30, batch 512/824, loss=0.1504959911108017\n",
            "saving model\n",
            "saved model\n",
            "training for batch 544/824\n",
            "training for batch 576/824\n",
            "training for batch 608/824\n",
            "training for batch 640/824\n",
            "epoch 22/30, batch 640/824, loss=0.1718325912952423\n",
            "saving model\n",
            "saved model\n",
            "training for batch 672/824\n",
            "training for batch 704/824\n",
            "training for batch 736/824\n",
            "training for batch 768/824\n",
            "epoch 22/30, batch 768/824, loss=0.04616686701774597\n",
            "saving model\n",
            "saved model\n",
            "training for batch 800/824\n",
            "epoch 22, training_loss = 0.4614927209230467\n",
            "saving model\n",
            "saved model\n",
            "training for batch 32/824\n",
            "training for batch 64/824\n",
            "training for batch 96/824\n",
            "training for batch 128/824\n",
            "epoch 23/30, batch 128/824, loss=3.4239840507507324\n",
            "saving model\n",
            "saved model\n",
            "training for batch 160/824\n",
            "training for batch 192/824\n",
            "training for batch 224/824\n",
            "training for batch 256/824\n",
            "epoch 23/30, batch 256/824, loss=0.4558824896812439\n",
            "saving model\n",
            "saved model\n",
            "training for batch 288/824\n",
            "training for batch 320/824\n",
            "training for batch 352/824\n",
            "training for batch 384/824\n",
            "epoch 23/30, batch 384/824, loss=0.0026966333389282227\n",
            "saving model\n",
            "saved model\n",
            "training for batch 416/824\n",
            "training for batch 448/824\n",
            "training for batch 480/824\n",
            "training for batch 512/824\n",
            "epoch 23/30, batch 512/824, loss=0.17510764300823212\n",
            "saving model\n",
            "saved model\n",
            "training for batch 544/824\n",
            "training for batch 576/824\n",
            "training for batch 608/824\n",
            "training for batch 640/824\n",
            "epoch 23/30, batch 640/824, loss=0.27698493003845215\n",
            "saving model\n",
            "saved model\n",
            "training for batch 672/824\n",
            "training for batch 704/824\n",
            "training for batch 736/824\n",
            "training for batch 768/824\n",
            "epoch 23/30, batch 768/824, loss=0.1094319224357605\n",
            "saving model\n",
            "saved model\n",
            "training for batch 800/824\n",
            "epoch 23, training_loss = 0.6673601094080743\n",
            "saving model\n",
            "saved model\n",
            "training for batch 32/824\n",
            "training for batch 64/824\n",
            "training for batch 96/824\n",
            "training for batch 128/824\n",
            "epoch 24/30, batch 128/824, loss=1.4557185173034668\n",
            "saving model\n",
            "saved model\n",
            "training for batch 160/824\n",
            "training for batch 192/824\n",
            "training for batch 224/824\n",
            "training for batch 256/824\n",
            "epoch 24/30, batch 256/824, loss=0.2835305333137512\n",
            "saving model\n",
            "saved model\n",
            "training for batch 288/824\n",
            "training for batch 320/824\n",
            "training for batch 352/824\n",
            "training for batch 384/824\n",
            "epoch 24/30, batch 384/824, loss=0.001571059226989746\n",
            "saving model\n",
            "saved model\n",
            "training for batch 416/824\n",
            "training for batch 448/824\n",
            "training for batch 480/824\n",
            "training for batch 512/824\n",
            "epoch 24/30, batch 512/824, loss=0.13622362911701202\n",
            "saving model\n",
            "saved model\n",
            "training for batch 544/824\n",
            "training for batch 576/824\n",
            "training for batch 608/824\n",
            "training for batch 640/824\n",
            "epoch 24/30, batch 640/824, loss=0.2303662747144699\n",
            "saving model\n",
            "saved model\n",
            "training for batch 672/824\n",
            "training for batch 704/824\n",
            "training for batch 736/824\n",
            "training for batch 768/824\n",
            "epoch 24/30, batch 768/824, loss=0.13135257363319397\n",
            "saving model\n",
            "saved model\n",
            "training for batch 800/824\n",
            "epoch 24, training_loss = 0.4540738009791476\n",
            "saving model\n",
            "saved model\n",
            "training for batch 32/824\n",
            "training for batch 64/824\n",
            "training for batch 96/824\n",
            "training for batch 128/824\n",
            "epoch 25/30, batch 128/824, loss=1.7912541627883911\n",
            "saving model\n",
            "saved model\n",
            "training for batch 160/824\n",
            "training for batch 192/824\n",
            "training for batch 224/824\n",
            "training for batch 256/824\n",
            "epoch 25/30, batch 256/824, loss=0.8648686408996582\n",
            "saving model\n",
            "saved model\n",
            "training for batch 288/824\n",
            "training for batch 320/824\n",
            "training for batch 352/824\n",
            "training for batch 384/824\n",
            "epoch 25/30, batch 384/824, loss=0.00025463104248046875\n",
            "saving model\n",
            "saved model\n",
            "training for batch 416/824\n",
            "training for batch 448/824\n",
            "training for batch 480/824\n",
            "training for batch 512/824\n",
            "epoch 25/30, batch 512/824, loss=0.1401987373828888\n",
            "saving model\n",
            "saved model\n",
            "training for batch 544/824\n",
            "training for batch 576/824\n",
            "training for batch 608/824\n",
            "training for batch 640/824\n",
            "epoch 25/30, batch 640/824, loss=0.13507793843746185\n",
            "saving model\n",
            "saved model\n",
            "training for batch 672/824\n",
            "training for batch 704/824\n",
            "training for batch 736/824\n",
            "training for batch 768/824\n",
            "epoch 25/30, batch 768/824, loss=0.06801101565361023\n",
            "saving model\n",
            "saved model\n",
            "training for batch 800/824\n",
            "epoch 25, training_loss = 0.539048637251834\n",
            "saving model\n",
            "saved model\n",
            "training for batch 32/824\n",
            "training for batch 64/824\n",
            "training for batch 96/824\n",
            "training for batch 128/824\n",
            "epoch 26/30, batch 128/824, loss=1.6735652685165405\n",
            "saving model\n",
            "saved model\n",
            "training for batch 160/824\n",
            "training for batch 192/824\n",
            "training for batch 224/824\n",
            "training for batch 256/824\n",
            "epoch 26/30, batch 256/824, loss=1.1788073778152466\n",
            "saving model\n",
            "saved model\n",
            "training for batch 288/824\n",
            "training for batch 320/824\n",
            "training for batch 352/824\n",
            "training for batch 384/824\n",
            "epoch 26/30, batch 384/824, loss=0.07135027647018433\n",
            "saving model\n",
            "saved model\n",
            "training for batch 416/824\n",
            "training for batch 448/824\n",
            "training for batch 480/824\n",
            "training for batch 512/824\n",
            "epoch 26/30, batch 512/824, loss=0.16366034746170044\n",
            "saving model\n",
            "saved model\n",
            "training for batch 544/824\n",
            "training for batch 576/824\n",
            "training for batch 608/824\n",
            "training for batch 640/824\n",
            "epoch 26/30, batch 640/824, loss=0.1340251863002777\n",
            "saving model\n",
            "saved model\n",
            "training for batch 672/824\n",
            "training for batch 704/824\n",
            "training for batch 736/824\n",
            "training for batch 768/824\n",
            "epoch 26/30, batch 768/824, loss=0.12383756041526794\n",
            "saving model\n",
            "saved model\n",
            "training for batch 800/824\n",
            "epoch 26, training_loss = 0.4711282896931181\n",
            "saving model\n",
            "saved model\n",
            "training for batch 32/824\n",
            "training for batch 64/824\n",
            "training for batch 96/824\n",
            "training for batch 128/824\n",
            "epoch 27/30, batch 128/824, loss=1.4860228300094604\n",
            "saving model\n",
            "saved model\n",
            "training for batch 160/824\n",
            "training for batch 192/824\n",
            "training for batch 224/824\n",
            "training for batch 256/824\n",
            "epoch 27/30, batch 256/824, loss=0.49780815839767456\n",
            "saving model\n",
            "saved model\n",
            "training for batch 288/824\n",
            "training for batch 320/824\n",
            "training for batch 352/824\n",
            "training for batch 384/824\n",
            "epoch 27/30, batch 384/824, loss=1.4305177273854497e-06\n",
            "saving model\n",
            "saved model\n",
            "training for batch 416/824\n",
            "training for batch 448/824\n",
            "training for batch 480/824\n",
            "training for batch 512/824\n",
            "epoch 27/30, batch 512/824, loss=0.05904921889305115\n",
            "saving model\n",
            "saved model\n",
            "training for batch 544/824\n",
            "training for batch 576/824\n",
            "training for batch 608/824\n",
            "training for batch 640/824\n",
            "epoch 27/30, batch 640/824, loss=0.22747327387332916\n",
            "saving model\n",
            "saved model\n",
            "training for batch 672/824\n",
            "training for batch 704/824\n",
            "training for batch 736/824\n",
            "training for batch 768/824\n",
            "epoch 27/30, batch 768/824, loss=0.09692361950874329\n",
            "saving model\n",
            "saved model\n",
            "training for batch 800/824\n",
            "epoch 27, training_loss = 0.5221903140911719\n",
            "saving model\n",
            "saved model\n",
            "training for batch 32/824\n",
            "training for batch 64/824\n",
            "training for batch 96/824\n",
            "training for batch 128/824\n",
            "epoch 28/30, batch 128/824, loss=1.195809006690979\n",
            "saving model\n",
            "saved model\n",
            "training for batch 160/824\n",
            "training for batch 192/824\n",
            "training for batch 224/824\n",
            "training for batch 256/824\n",
            "epoch 28/30, batch 256/824, loss=0.5757898688316345\n",
            "saving model\n",
            "saved model\n",
            "training for batch 288/824\n",
            "training for batch 320/824\n",
            "training for batch 352/824\n",
            "training for batch 384/824\n",
            "epoch 28/30, batch 384/824, loss=1.3204418298790738e-13\n",
            "saving model\n",
            "saved model\n",
            "training for batch 416/824\n",
            "training for batch 448/824\n",
            "training for batch 480/824\n",
            "training for batch 512/824\n",
            "epoch 28/30, batch 512/824, loss=0.024202167987823486\n",
            "saving model\n",
            "saved model\n",
            "training for batch 544/824\n",
            "training for batch 576/824\n",
            "training for batch 608/824\n",
            "training for batch 640/824\n",
            "epoch 28/30, batch 640/824, loss=0.29274362325668335\n",
            "saving model\n",
            "saved model\n",
            "training for batch 672/824\n",
            "training for batch 704/824\n",
            "training for batch 736/824\n",
            "training for batch 768/824\n",
            "epoch 28/30, batch 768/824, loss=0.04631194472312927\n",
            "saving model\n",
            "saved model\n",
            "training for batch 800/824\n",
            "epoch 28, training_loss = 0.40632218190280645\n",
            "saving model\n",
            "saved model\n",
            "training for batch 32/824\n",
            "training for batch 64/824\n",
            "training for batch 96/824\n",
            "training for batch 128/824\n",
            "epoch 29/30, batch 128/824, loss=0.7264852523803711\n",
            "saving model\n",
            "saved model\n",
            "training for batch 160/824\n",
            "training for batch 192/824\n",
            "training for batch 224/824\n",
            "training for batch 256/824\n",
            "epoch 29/30, batch 256/824, loss=4.893506050109863\n",
            "saving model\n",
            "saved model\n",
            "training for batch 288/824\n",
            "training for batch 320/824\n",
            "training for batch 352/824\n",
            "training for batch 384/824\n",
            "epoch 29/30, batch 384/824, loss=0.0009714365005493164\n",
            "saving model\n",
            "saved model\n",
            "training for batch 416/824\n",
            "training for batch 448/824\n",
            "training for batch 480/824\n",
            "training for batch 512/824\n",
            "epoch 29/30, batch 512/824, loss=0.10091079771518707\n",
            "saving model\n",
            "saved model\n",
            "training for batch 544/824\n",
            "training for batch 576/824\n",
            "training for batch 608/824\n",
            "training for batch 640/824\n",
            "epoch 29/30, batch 640/824, loss=0.5237495303153992\n",
            "saving model\n",
            "saved model\n",
            "training for batch 672/824\n",
            "training for batch 704/824\n",
            "training for batch 736/824\n",
            "training for batch 768/824\n",
            "epoch 29/30, batch 768/824, loss=0.18676993250846863\n",
            "saving model\n",
            "saved model\n",
            "training for batch 800/824\n",
            "epoch 29, training_loss = 0.4219003131347768\n",
            "saving model\n",
            "saved model\n",
            "training for batch 32/824\n",
            "training for batch 64/824\n",
            "training for batch 96/824\n",
            "training for batch 128/824\n",
            "epoch 30/30, batch 128/824, loss=1.0357046127319336\n",
            "saving model\n",
            "saved model\n",
            "training for batch 160/824\n",
            "training for batch 192/824\n",
            "training for batch 224/824\n",
            "training for batch 256/824\n",
            "epoch 30/30, batch 256/824, loss=0.31026244163513184\n",
            "saving model\n",
            "saved model\n",
            "training for batch 288/824\n",
            "training for batch 320/824\n",
            "training for batch 352/824\n",
            "training for batch 384/824\n",
            "epoch 30/30, batch 384/824, loss=1.1920928955078125e-06\n",
            "saving model\n",
            "saved model\n",
            "training for batch 416/824\n",
            "training for batch 448/824\n",
            "training for batch 480/824\n",
            "training for batch 512/824\n",
            "epoch 30/30, batch 512/824, loss=0.08829161524772644\n",
            "saving model\n",
            "saved model\n",
            "training for batch 544/824\n",
            "training for batch 576/824\n",
            "training for batch 608/824\n",
            "training for batch 640/824\n",
            "epoch 30/30, batch 640/824, loss=0.09292973577976227\n",
            "saving model\n",
            "saved model\n",
            "training for batch 672/824\n",
            "training for batch 704/824\n",
            "training for batch 736/824\n",
            "training for batch 768/824\n",
            "epoch 30/30, batch 768/824, loss=0.167586088180542\n",
            "saving model\n",
            "saved model\n",
            "training for batch 800/824\n",
            "epoch 30, training_loss = 0.1814091098967118\n",
            "saving model\n",
            "saved model\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "n_epochs = 30\n",
        "n_samples = len(train_dataset)\n",
        "n_iterations = ceil(n_samples / batch_size)\n",
        "\n",
        "input_dim = (3, 90, 240, 320)\n",
        "model = ConvNet3D(input_dim)\n",
        "\n",
        "# Calculate pos_weight based on class imbalance\n",
        "pos_weight = torch.tensor([1910 / 231]) #manually counted\n",
        "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight.to(device))\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
        "#scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10)\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "# Load a previously saved checkpoint (if resuming)\n",
        "checkpoint_file = 'third_model_checkpoint.pth'\n",
        "checkpoint_dir = '/content/drive/Shareddrives/thesis/models/checkpoints/'\n",
        "checkpoint_path = checkpoint_dir + checkpoint_file\n",
        "!mkdir -p $checkpoint_dir\n",
        "start_epoch = 0\n",
        "start_batch = 0\n",
        "epoch_loss = [] #loss for each batch\n",
        "train_loss = [] #gets average of epoch_loss every epoch\n",
        "try:\n",
        "    checkpoint = torch.load(checkpoint_path)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    #scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "    start_epoch = checkpoint['epoch']\n",
        "    start_batch = checkpoint['batch']\n",
        "    train_loss = checkpoint['train_loss']\n",
        "    epoch_loss = checkpoint['epoch_loss']\n",
        "    print(f\"Resuming training from epoch {start_epoch + 1}, batch {start_batch}\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Starting training from scratch.\")\n",
        "\n",
        "# clips_without_frames = set()\n",
        "# clips_without_frames.add\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(start_epoch, n_epochs):\n",
        "  #current_lr = scheduler.get_last_lr()[0]\n",
        "  #print(f\"Current Learning Rate: {current_lr}\")\n",
        "  for i, (_, inputs, labels) in enumerate(train_dataloader):\n",
        "    if epoch == start_epoch and i < start_batch:\n",
        "      if i % 32 == 0:\n",
        "        print(f\"skipping batch {i}\")\n",
        "      continue\n",
        "    if (i + 1) % 32 == 0: print(f\"training for batch {i + 1}/{n_iterations}\")\n",
        "\n",
        "    inputs = inputs.to(device)\n",
        "    labels = labels.to(device)\n",
        "    preds = model(inputs)\n",
        "\n",
        "    # calculate loss\n",
        "    optimizer.zero_grad()\n",
        "    loss = criterion(preds, labels)\n",
        "    epoch_loss.append(loss.item())\n",
        "\n",
        "    #update weights\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Save a checkpoint every 128 batches\n",
        "    if (i + 1) % 128 == 0:\n",
        "      print(f\"epoch {epoch + 1}/{n_epochs}, batch {i + 1}/{n_iterations}, loss={loss.item()}\")\n",
        "      print(f\"saving model\")\n",
        "      torch.save({\n",
        "          'epoch': epoch,\n",
        "          'batch': i + 1,\n",
        "          'model_state_dict': model.state_dict(),\n",
        "          'optimizer_state_dict': optimizer.state_dict(),\n",
        "          'epoch_loss': epoch_loss,\n",
        "          'train_loss': train_loss,\n",
        "          # 'scheduler_state_dict': scheduler.state_dict(),\n",
        "      }, checkpoint_path)\n",
        "      print(\"saved model\")\n",
        "\n",
        "\n",
        "  # save after epoch\n",
        "  train_loss.append(sum(epoch_loss) / len(epoch_loss))\n",
        "  epoch_loss = []\n",
        "  print(f\"epoch {epoch + 1}, training_loss = {train_loss[-1]}\")\n",
        "  print(\"saving model\")\n",
        "  torch.save({\n",
        "            'epoch': epoch + 1,\n",
        "            'batch': 0,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'epoch_loss': [],\n",
        "            'train_loss': train_loss,\n",
        "            #'scheduler_state_dict': scheduler.state_dict(),\n",
        "        }, checkpoint_path)\n",
        "  # scheduler.step(loss)\n",
        "  print(\"saved model\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nv_2sZp5LRAl"
      },
      "source": [
        "## Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n5_piCOQfpZg",
        "outputId": "0945c928-8247-42a7-d051-68c757cf7df9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-0ee319781b84>:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(checkpoint_path)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "testing for batch 10/134\n",
            "testing for batch 20/134\n",
            "testing for batch 30/134\n",
            "testing for batch 40/134\n",
            "testing for batch 50/134\n",
            "testing for batch 60/134\n",
            "testing for batch 70/134\n",
            "testing for batch 80/134\n",
            "testing for batch 90/134\n",
            "testing for batch 100/134\n",
            "testing for batch 110/134\n",
            "testing for batch 120/134\n",
            "testing for batch 130/134\n",
            "tested all 134 batches\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "# validation\n",
        "input_dim = (3, 90, 240, 320)\n",
        "model = ConvNet3D(input_dim)\n",
        "threshold = 0.4\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# Load model\n",
        "checkpoint_file = 'third_model_checkpoint.pth'\n",
        "checkpoint_path = '/content/drive/Shareddrives/thesis/models/checkpoints/' + checkpoint_file\n",
        "checkpoint = torch.load(checkpoint_path)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "model = model.to(device)\n",
        "\n",
        "indices_all = []\n",
        "y_true_all = []\n",
        "y_pred_all = []\n",
        "y_prob_all = []\n",
        "n_iterations = ceil(len(test_dataset) / batch_size)\n",
        "\n",
        "with torch.no_grad():\n",
        "  for batch, (indices, inputs, labels) in enumerate(test_dataloader):\n",
        "    if (batch + 1) % 10 == 0: print(f\"testing for batch {batch + 1}/{n_iterations}\")\n",
        "    inputs = inputs.to(device)\n",
        "\n",
        "    outputs = torch.sigmoid(model(inputs))\n",
        "    rounded = (outputs > threshold).int()\n",
        "\n",
        "    indices_all.extend(indices.numpy())\n",
        "    y_true_all.extend(labels.numpy())\n",
        "    y_prob_all.extend(outputs.cpu().numpy())\n",
        "    y_pred_all.extend(rounded.cpu().numpy())\n",
        "  print(f'tested all {n_iterations} batches')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QbFoYQpeAE0r",
        "outputId": "34c3f3b6-d459-4ad1-db85-b9de1ea73ba2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8209\n",
            "Precision: 0.3173\n",
            "Recall: 0.5690\n",
            "F1 Score: 0.4074\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-19-364c59edf9ce>:35: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
            "  metrics = calculate_metrics(torch.tensor(indices_all), torch.tensor(y_pred_all), torch.tensor(y_true_all))\n"
          ]
        }
      ],
      "source": [
        "false_positives = []\n",
        "false_negatives = []\n",
        "true_positives = []\n",
        "true_negatives = []\n",
        "\n",
        "def calculate_metrics(inds, preds, labels):\n",
        "    TP = (preds == 1) & (labels == 1)\n",
        "    TN = (preds == 0) & (labels == 0)\n",
        "    FP = (preds == 1) & (labels == 0)\n",
        "    FN = (preds == 0) & (labels == 1)\n",
        "\n",
        "    true_positives.extend(inds[TP].tolist())\n",
        "    true_negatives.extend(inds[TN].tolist())\n",
        "    false_positives.extend(inds[FP].tolist())\n",
        "    false_negatives.extend(inds[FN].tolist())\n",
        "\n",
        "    TP = torch.sum(TP).item()\n",
        "    TN = torch.sum(TN).item()\n",
        "    FP = torch.sum(FP).item()\n",
        "    FN = torch.sum(FN).item()\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = (TP + TN) / (TP + TN + FP + FN) if (TP + TN + FP + FN) != 0 else 0\n",
        "    precision = TP / (TP + FP) if (TP + FP) != 0 else 0\n",
        "    recall = TP / (TP + FN) if (TP + FN) != 0 else 0\n",
        "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0\n",
        "\n",
        "    return {\n",
        "        \"accuracy\": accuracy,\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall,\n",
        "        \"f1\": f1\n",
        "    }\n",
        "\n",
        "metrics = calculate_metrics(torch.tensor(indices_all), torch.tensor(y_pred_all), torch.tensor(y_true_all))\n",
        "print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n",
        "print(f\"Precision: {metrics['precision']:.4f}\")\n",
        "print(f\"Recall: {metrics['recall']:.4f}\")\n",
        "print(f\"F1 Score: {metrics['f1']:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PdqcARyqB0tx",
        "outputId": "de1a0f2f-f6f0-403c-f63b-54b0f5175531"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[tensor(0.0500), 0.3173076923076923]\n",
            "[tensor(0.0500), 0.8208955223880597]\n",
            "[tensor(0.0500), 0.5689655172413793]\n",
            "[tensor(0.0500), 0.4074074074074074]\n"
          ]
        }
      ],
      "source": [
        "precision = [0,0]\n",
        "accuracy = [0,0]\n",
        "recall = [0,0]\n",
        "f1 = [0,0]\n",
        "for threshold in torch.arange(0.05, 1, 0.05):\n",
        "  y_pred_test = (torch.tensor(y_prob_all) > threshold).int()\n",
        "  metrics = calculate_metrics(torch.tensor(indices_all), torch.tensor(y_pred_all), torch.tensor(y_true_all))\n",
        "  if precision[1] < metrics['precision']:\n",
        "    precision = [threshold, metrics['precision']]\n",
        "  if accuracy[1] < metrics['accuracy']:\n",
        "    accuracy = [threshold, metrics['accuracy']]\n",
        "  if recall[1] < metrics['recall']:\n",
        "    recall = [threshold, metrics['recall']]\n",
        "  if f1[1] < metrics['f1']:\n",
        "    f1 = [threshold, metrics['f1']]\n",
        "print(precision)\n",
        "print(accuracy)\n",
        "print(recall)\n",
        "print(f1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3IZu6YfdqGk7",
        "outputId": "d5744d94-af4f-4aa3-95d8-0ee9f53fa5d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.30212513 1.0\n"
          ]
        }
      ],
      "source": [
        "y_prob_all = np.array(y_prob_all)\n",
        "print(y_prob_all.mean(), y_prob_all.max())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o3e4ScUVIXLQ",
        "outputId": "7d14e933-da37-4667-dfbe-c0a3d6442e31"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True Positives: \n",
            "1     Shoplifting028_x264_4.mp4\n",
            "35    Shoplifting026_x264_4.mp4\n",
            "37    Shoplifting013_x264_7.mp4\n",
            "62    Shoplifting018_x264_3.mp4\n",
            "80    Shoplifting018_x264_9.mp4\n",
            "Name: Filename, dtype: object\n",
            "False Negatives: \n",
            "36    Shoplifting040_x264_16.mp4\n",
            "52     Shoplifting029_x264_9.mp4\n",
            "86    Shoplifting043_x264_51.mp4\n",
            "94    Shoplifting034_x264_54.mp4\n",
            "97     Shoplifting037_x264_9.mp4\n",
            "Name: Filename, dtype: object\n",
            "False Positives\n",
            "26    Shoplifting041_x264_11.mp4\n",
            "46    Shoplifting041_x264_69.mp4\n",
            "57    Shoplifting048_x264_10.mp4\n",
            "83     Shoplifting055_x264_2.mp4\n",
            "91    Shoplifting003_x264_76.mp4\n",
            "Name: Filename, dtype: object\n"
          ]
        }
      ],
      "source": [
        "base_path = \"/content/drive/Shareddrives/thesis/dataset/\"\n",
        "file_path = \"test.csv\"\n",
        "data = pd.read_csv(base_path + file_path)['Filename']\n",
        "false_positive_names = data.iloc[false_positives]\n",
        "false_negative_names = data.iloc[false_negatives]\n",
        "true_positive_names = data.iloc[true_positives]\n",
        "true_negative_names = data.iloc[true_negatives]\n",
        "print('True Positives: ')\n",
        "print(true_positive_names.head())\n",
        "print('False Negatives: ')\n",
        "print(false_negative_names.head())\n",
        "print('False Positives')\n",
        "print(false_positive_names.head())\n",
        "\n",
        "with open(f\"{checkpoint_path.removesuffix('.pth')}_predictions.txt\", 'w') as f:\n",
        "  f.write(\"False Positives: \\n\")\n",
        "  for item in false_positive_names:\n",
        "    f.write(f\"{item}\\n\")\n",
        "  f.write(\"\\nFalse Negatives: \\n\")\n",
        "  for item in false_negative_names:\n",
        "    f.write(f\"{item}\\n\")\n",
        "  f.write(\"\\nTrue Positives: \\n\")\n",
        "  for item in true_positive_names:\n",
        "    f.write(f\"{item}\\n\")\n",
        "  f.write(\"\\nTrue Negatives: \\n\")\n",
        "  for item in true_negative_names:\n",
        "    f.write(f\"{item}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "-M1p9xSuBPQ6",
        "outputId": "4c6d18e5-71c3-4df1-c0f9-2663057dc065"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7ee774770c10>]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "training_loss = checkpoint['train_loss']\n",
        "plt.plot(training_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "IwTrbfw-3823",
        "outputId": "9312aba8-64a3-4773-aad4-83785b0a2ced"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Calculate the confusion matrix\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "\n",
        "cm = confusion_matrix(y_true_all, y_pred_all)\n",
        "\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "\n",
        "disp.plot()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Timing"
      ],
      "metadata": {
        "id": "xWpvTYHkhXle"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "# validation\n",
        "input_dim = (3, 90, 240, 320)\n",
        "model = ConvNet3D(input_dim)\n",
        "threshold = 0.4\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# Load model\n",
        "checkpoint_file = 'third_model_checkpoint.pth'\n",
        "checkpoint_path = '/content/drive/Shareddrives/thesis/models/checkpoints/' + checkpoint_file\n",
        "checkpoint = torch.load(checkpoint_path)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NYEYsE8PhVqv",
        "outputId": "cd01699a-81b9-4dec-906b-5e56d0a6ecdb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-51490f4fb012>:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(checkpoint_path)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import cv2\n",
        "import os\n",
        "\n",
        "class BaseDataset(Dataset):\n",
        "  def __init__(self, frames_per_clip=90, train=True, transform=None, source='Original', lstm_format=False):\n",
        "    if source not in ['Original', 'YOLO', 'Openpose']:\n",
        "      raise RuntimeError(\"Invalid source. Should be one of 'Original', 'YOLO', or 'Openpose'\")\n",
        "    #load data\n",
        "    base_path = \"/content/drive/Shareddrives/thesis/dataset/\"\n",
        "    file_path = \"train.csv\" if train else \"test.csv\"\n",
        "    self.fpc = frames_per_clip\n",
        "    self.data = pd.read_csv(base_path + file_path)\n",
        "    self.targets = self.data[\"Label\"].values\n",
        "    self.transform = transform\n",
        "    self.source = source\n",
        "    self.lstm_format = lstm_format\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "  def make_tensor(self, frames):\n",
        "    #To do: normalize frames\n",
        "    frames_tensor = torch.stack([torch.from_numpy(frame) for frame in frames])\n",
        "    #frames have shape (n_frames, width, height, channels)\n",
        "    if self.lstm_format:\n",
        "      frames_tensor = frames_tensor.permute(0,3,1,2)\n",
        "    else:\n",
        "      frames_tensor = frames_tensor.permute(3,0,1,2)\n",
        "    frames_tensor = frames_tensor.type(torch.float32)\n",
        "    if self.transform:\n",
        "      frames_tensor = self.transform(frames_tensor)\n",
        "    return frames_tensor\n",
        "\n",
        "class ClipDataset(BaseDataset):\n",
        "  def __getitem__(self, x):\n",
        "      frames = []\n",
        "      if self.source == 'YOLO':\n",
        "        base_path = \"/content/drive/Shareddrives/shoplifting-detection/pose_estimation_results/YOLO\"\n",
        "        file_path = os.path.join(base_path, self.data.loc[x, \"Parent Video\"], self.data.loc[x, \"Filename\"].removesuffix(\".mp4\"), self.data.loc[x, \"Filename\"].removesuffix(\".mp4\") +\"+poses.mp4\")\n",
        "      else:\n",
        "        base_path = \"/content/drive/Shareddrives/thesis/dataset\"\n",
        "        file_path = os.path.join(base_path, \"split\", \"Shoplifting\", self.data.loc[x, \"Parent Video\"], self.data.loc[x, \"Filename\"])\n",
        "\n",
        "      vidcap = cv2.VideoCapture(file_path)\n",
        "      total_frames = int(vidcap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "      # total_frames = self.data.loc[x, \"Frame Count\"]\n",
        "      frames_indexes = np.linspace(0, total_frames - 1, self.fpc, dtype=int)\n",
        "\n",
        "      for ind in frames_indexes:\n",
        "        vidcap.set(cv2.CAP_PROP_POS_FRAMES, ind)\n",
        "        success, image = vidcap.read()\n",
        "        if success:\n",
        "          frames.append(image)\n",
        "        else:\n",
        "          print(f\"frame_{ind}.jpg cannot be found for {self.data.loc[x, 'Filename']}\")\n",
        "      vidcap.release()\n",
        "\n",
        "      if frames == [] or len(frames) < self.fpc:\n",
        "        # no frames found in folder\n",
        "        raise ValueError(f\"{total_frames - len(frames)} missing frames for {self.data.loc[x, 'Filename']}\")\n",
        "\n",
        "      frames_tensor = self.make_tensor(frames)\n",
        "      label = torch.tensor([self.data.loc[x, \"Label\"]], dtype=torch.float32)\n",
        "      index = torch.tensor([x])\n",
        "      return index, frames_tensor, label\n"
      ],
      "metadata": {
        "id": "4wf3Fwzjl1W3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = ClipDataset(train=False, source='YOLO')\n",
        "_, frames, _ = test_dataset[20]\n",
        "frames = frames.to(device).unsqueeze(0)\n",
        "frames.shape"
      ],
      "metadata": {
        "id": "IIZ-QO1cmto5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6693572-a501-4302-c93e-4f6139b6e9b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 3, 90, 240, 320])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%timeit\n",
        "with torch.inference_mode():\n",
        "  output = model(frames)\n",
        "  torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "O6hMqpGchg8F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7368d9c6-5434-49d1-9a8a-718c7840860c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "53.6 ms ± 192 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}