{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZD5ugAvrWgC"
      },
      "source": [
        "Check first if a GPU is installed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4hFtngnzraTi",
        "outputId": "191de125-745b-4863-8735-6da5d35d45d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tue Jan 21 13:45:51 2025       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  NVIDIA A100-SXM4-40GB          Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   32C    P0              46W / 400W |      2MiB / 40960MiB |      0%      Default |\n",
            "|                                         |                      |             Disabled |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kj22DNHZrkvg"
      },
      "source": [
        "Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yRqVkBkSrkdz",
        "outputId": "60c22bec-00a0-481d-ce2d-37fff828f241"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkNZ7dpV6kCe"
      },
      "source": [
        "### Training Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ujJiYExfXJwL"
      },
      "outputs": [],
      "source": [
        "# import numpy as np\n",
        "# import pandas as pd\n",
        "# import os\n",
        "\n",
        "# base_path = \"/content/drive/Shareddrives/thesis/dataset/\"\n",
        "\n",
        "# clips = pd.read_csv(base_path + \"Summary of Dataset Information - Clips.csv\")\n",
        "\n",
        "# def get_frame_count(path):\n",
        "#   count = os.popen(f\"ffprobe -v error -select_streams v:0 -count_frames -show_entries stream=nb_read_frames -of csv=p=0 {path}\").read()\n",
        "#   return count.strip()\n",
        "\n",
        "# clips[\"Full Path\"] = base_path + \"split/Shoplifting/\" + clips[\"Parent Video\"] + \"/\" + clips[\"Filename\"]\n",
        "# clips[\"Frame Count\"] = clips[\"Full Path\"].apply(get_frame_count)\n",
        "# print(clips[\"Frame Count\"].head())\n",
        "\n",
        "# #remove clips with fewer than 120 frames\n",
        "# filtered_clips = clips[clips[\"Frame Count\"] == '120']\n",
        "\n",
        "# print(clips.shape[0])\n",
        "# print(filtered_clips.shape[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sHLDaBQHYzjY"
      },
      "outputs": [],
      "source": [
        "# from sklearn.model_selection import train_test_split\n",
        "\n",
        "# filtered_clips = filtered_clips.drop(columns=[\"Full Path\"])\n",
        "\n",
        "# train, test = train_test_split(filtered_clips, test_size=0.2, stratify=filtered_clips[\"Label\"], random_state=71)\n",
        "\n",
        "# #TODO: make new coptadd augmented clips\n",
        "\n",
        "# print(train.shape[0])\n",
        "# print(test.shape[0])\n",
        "\n",
        "# #save validation and train split\n",
        "# test.to_csv(base_path + \"test.csv\", index=False)\n",
        "# train.to_csv(base_path + \"train.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SfgxlO5AnNi4"
      },
      "source": [
        "##Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enz5r0bkUI_8"
      },
      "source": [
        "##Data Loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "OYAFiiczSBmS"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import cv2\n",
        "import os\n",
        "\n",
        "class BaseDataset(Dataset):\n",
        "  def __init__(self, frames_per_clip=90, train=True, transform=None, source='Original', lstm_format=False):\n",
        "    if source not in ['Original', 'YOLO', 'Openpose']:\n",
        "      raise RuntimeError(\"Invalid source. Should be one of 'Original', 'YOLO', or 'Openpose'\")\n",
        "    #load data\n",
        "    base_path = \"/content/drive/Shareddrives/thesis/dataset/\"\n",
        "    file_path = \"train.csv\" if train else \"test.csv\"\n",
        "    self.fpc = frames_per_clip\n",
        "    self.data = pd.read_csv(base_path + file_path)\n",
        "    self.targets = self.data[\"Label\"].values\n",
        "    self.transform = transform\n",
        "    self.source = source\n",
        "    self.lstm_format = lstm_format\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "  def make_tensor(self, frames):\n",
        "    #To do: normalize frames\n",
        "    frames_tensor = torch.stack([torch.from_numpy(frame) for frame in frames])\n",
        "    #frames have shape (n_frames, width, height, channels)\n",
        "    if self.lstm_format:\n",
        "      frames_tensor = frames_tensor.permute(0,3,1,2)\n",
        "    else:\n",
        "      frames_tensor = frames_tensor.permute(3,0,1,2)\n",
        "    frames_tensor = frames_tensor.type(torch.float32)\n",
        "    if self.transform:\n",
        "      frames_tensor = self.transform(frames_tensor)\n",
        "    return frames_tensor\n",
        "\n",
        "\n",
        "class FrameDataset(BaseDataset):\n",
        "  def __getitem__(self, x):\n",
        "      frames = []\n",
        "        # for frames\n",
        "      if self.source == 'YOLO':\n",
        "        # base_path = \"/content/drive/Shareddrives/shoplifting-detection/pose_estimation_results/YOLO\"\n",
        "        base_path = \"/content/drive/Shareddrives/shoplifting-dataset/pose_estimation_results_70/YOLOv70\"\n",
        "        frames_path = os.path.join(base_path, self.data.loc[x, \"Parent Video\"], self.data.loc[x, \"Filename\"].removesuffix(\".mp4\"), \"frames\")\n",
        "      else:\n",
        "        base_path = \"/content/drive/Shareddrives/shoplifting-detection/dataset/Frames/Shoplifting\"\n",
        "        frames_path = os.path.join(base_path, self.data.loc[x, \"Parent Video\"], self.data.loc[x, \"Filename\"])\n",
        "      total_frames = self.data.loc[x, \"Frame Count\"]\n",
        "      #total_frames = len(os.listdir(frames_path))\n",
        "      frames_indexes = np.linspace(0, total_frames - 1, self.fpc, dtype=int)\n",
        "\n",
        "      #in case of missing frames in YOLO output\n",
        "      split_path = os.path.join(\"/content/drive/Shareddrives/shoplifting-detection/dataset/Frames/Shoplifting\",\n",
        "                                  self.data.loc[x, \"Parent Video\"], self.data.loc[x, \"Filename\"])\n",
        "\n",
        "      for ind in frames_indexes:\n",
        "        image = cv2.imread(frames_path + f\"/frame_{ind}.jpg\")\n",
        "        if image is not None:\n",
        "          frames.append(image)\n",
        "        else:\n",
        "          image = cv2.imread(split_path + f\"/frame_{ind}.jpg\")\n",
        "          if image is not None:\n",
        "            frames.append(image)\n",
        "          else:\n",
        "            print(f\"frame_{ind}.jpg cannot be found for {self.data.loc[x, 'Filename']}\")\n",
        "\n",
        "      if frames == [] or len(frames) < self.fpc:\n",
        "        # no frames or some missing\n",
        "        raise ValueError(f\"{total_frames - len(frames)} missing frames for {self.data.loc[x, 'Filename']}\")\n",
        "\n",
        "      frames_tensor = self.make_tensor(frames)\n",
        "      label = torch.tensor([self.data.loc[x, \"Label\"]], dtype=torch.float32)\n",
        "      index = torch.tensor([x])\n",
        "      return index, frames_tensor, label\n",
        "\n",
        "class ClipDataset(BaseDataset):\n",
        "  def __getitem__(self, x):\n",
        "      frames = []\n",
        "      if self.source == 'YOLO':\n",
        "        base_path = \"/content/drive/Shareddrives/shoplifting-dataset/pose_estimation_results_70/YOLOv70\"\n",
        "        file_path = os.path.join(base_path, self.data.loc[x, \"Parent Video\"], self.data.loc[x, \"Filename\"].removesuffix(\".mp4\"), self.data.loc[x, \"Filename\"].removesuffix(\".mp4\") +\"+poses.mp4\")\n",
        "      else:\n",
        "        base_path = \"/content/drive/Shareddrives/thesis/dataset\"\n",
        "        file_path = os.path.join(base_path, \"split\", \"Shoplifting\", self.data.loc[x, \"Parent Video\"], self.data.loc[x, \"Filename\"])\n",
        "\n",
        "      vidcap = cv2.VideoCapture(file_path)\n",
        "      total_frames = int(vidcap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "      # total_frames = self.data.loc[x, \"Frame Count\"]\n",
        "      frames_indexes = np.linspace(0, total_frames - 1, self.fpc, dtype=int)\n",
        "\n",
        "      for ind in frames_indexes:\n",
        "        vidcap.set(cv2.CAP_PROP_POS_FRAMES, ind)\n",
        "        success, image = vidcap.read()\n",
        "        if success:\n",
        "          frames.append(image)\n",
        "        else:\n",
        "          print(f\"frame_{ind}.jpg cannot be found for {self.data.loc[x, 'Filename']}\")\n",
        "      vidcap.release()\n",
        "\n",
        "      if frames == [] or len(frames) < self.fpc:\n",
        "        # no frames found in folder\n",
        "        raise ValueError(f\"{total_frames - len(frames)} missing frames for {self.data.loc[x, 'Filename']}\")\n",
        "\n",
        "      frames_tensor = self.make_tensor(frames)\n",
        "      label = torch.tensor([self.data.loc[x, \"Label\"]], dtype=torch.float32)\n",
        "      index = torch.tensor([x])\n",
        "      return index, frames_tensor, label\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "A8UVUps1UhPV"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(1337) #for reproducibility\n",
        "batch_size = 4\n",
        "train_dataset = ClipDataset(train=True, source='YOLO')\n",
        "test_dataset = ClipDataset(train=False, source='YOLO')\n",
        "# train_dataset = FrameDataset(train=True, source='YOLO')\n",
        "# test_dataset = FrameDataset(train=False, source='YOLO')\n",
        "labels = torch.tensor(train_dataset.targets)\n",
        "\n",
        "train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, num_workers=4)\n",
        "test_dataloader = DataLoader(dataset=test_dataset, batch_size=batch_size, num_workers=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RDvjsSdESOuq",
        "outputId": "38917483-1979-4b68-bf90-db55ea7c0f72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1])\n",
            "torch.Size([3, 90, 240, 320])\n",
            "torch.Size([1])\n",
            "torch.Size([4, 1])\n",
            "torch.Size([4, 3, 90, 240, 320])\n",
            "torch.Size([4, 1])\n"
          ]
        }
      ],
      "source": [
        "name, image, label = train_dataset[0]\n",
        "print(name.shape)\n",
        "print(image.shape)\n",
        "print(label.shape)\n",
        "train_iter = iter(train_dataloader)\n",
        "names, images, labels = next(train_iter)\n",
        "print(names.shape)\n",
        "print(images.shape)\n",
        "print(labels.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMtbf0MVSRgm"
      },
      "source": [
        "##3D CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "4FYNvAssEVC6"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from math import ceil\n",
        "\n",
        "class ConvNet3D(nn.Module):\n",
        "\n",
        "  def __init__(self, input_dim, output_dim=1):\n",
        "    #Input should have shape (channels, frames, height, width)\n",
        "    super(ConvNet3D, self).__init__()\n",
        "    #Convolutional Layers\n",
        "    self.conv1 = nn.Conv3d(input_dim[0], 16, kernel_size=3, padding=1)\n",
        "    self.pool = nn.MaxPool3d(kernel_size=2, stride=2)\n",
        "    #self.norm1 = nn.BatchNorm3d(16)\n",
        "    self.conv2 = nn.Conv3d(16, 32, kernel_size=3, padding=1)\n",
        "    #self.norm2 = nn.BatchNorm3d(32)\n",
        "    self.conv3 = nn.Conv3d(32, 64, kernel_size=3, padding=1)\n",
        "    #self.norm3 = nn.BatchNorm3d(64)\n",
        "\n",
        "    #Fully Connected Layers\n",
        "    fc_input_size = 64 * (input_dim[1] // 8) * (input_dim[2] // 8) * (input_dim[3] // 8)\n",
        "    self.fc1 = nn.Linear(fc_input_size, 128)\n",
        "    self.fc2 = nn.Linear(128, 64)\n",
        "    self.fc3 = nn.Linear(64, output_dim)\n",
        "\n",
        "    self.sig = nn.Sigmoid()\n",
        "    self.dropout = nn.Dropout(p=0.5)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # out = self.pool(F.relu(self.norm1(self.conv1(x))))\n",
        "    # out = self.pool(F.relu(self.norm2(self.conv2(out))))\n",
        "    #out = self.dropout(out)\n",
        "    # out = self.pool(F.relu(self.norm3(self.conv3(out))))\n",
        "    out = self.pool(F.relu(self.conv1(x)))\n",
        "    out = self.pool(F.relu(self.conv2(out)))\n",
        "    out = self.pool(F.relu(self.conv3(out)))\n",
        "    fc_input_size = 64 * (input_dim[1] // 8) * (input_dim[2] // 8) * (input_dim[3] // 8)\n",
        "    out = out.view(-1, fc_input_size)\n",
        "    out = F.relu(self.fc1(out))\n",
        "    out = F.relu(self.fc2(out))\n",
        "    out = self.fc3(out)\n",
        "    return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ErnMwPxvLUSH"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aOIuKDhp3SQm",
        "outputId": "743f2fd1-6c6b-442a-974f-62e4956ec275"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting ipdb\n",
            "  Downloading ipdb-0.13.13-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: ipython>=7.31.1 in /usr/local/lib/python3.11/dist-packages (from ipdb) (7.34.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipdb) (4.4.2)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.11/dist-packages (from ipython>=7.31.1->ipdb) (75.1.0)\n",
            "Collecting jedi>=0.16 (from ipython>=7.31.1->ipdb)\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython>=7.31.1->ipdb) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.11/dist-packages (from ipython>=7.31.1->ipdb) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython>=7.31.1->ipdb) (3.0.50)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython>=7.31.1->ipdb) (2.18.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython>=7.31.1->ipdb) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.11/dist-packages (from ipython>=7.31.1->ipdb) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython>=7.31.1->ipdb) (4.9.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython>=7.31.1->ipdb) (0.8.4)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython>=7.31.1->ipdb) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.31.1->ipdb) (0.2.13)\n",
            "Downloading ipdb-0.13.13-py3-none-any.whl (12 kB)\n",
            "Downloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jedi, ipdb\n",
            "Successfully installed ipdb-0.13.13 jedi-0.19.2\n"
          ]
        }
      ],
      "source": [
        "!pip install ipdb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "aYV6DhU_h0Hb",
        "outputId": "ecc61c4b-e813-4852-f3c4-ae948742882d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-7-0a8dc4b0cdfa>:29: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(checkpoint_path)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Resuming training from epoch 1, batch 384\n",
            "skipping batch 0\n",
            "skipping batch 32\n",
            "skipping batch 64\n",
            "skipping batch 96\n",
            "skipping batch 128\n",
            "skipping batch 160\n",
            "skipping batch 192\n",
            "skipping batch 224\n",
            "skipping batch 256\n",
            "skipping batch 288\n",
            "skipping batch 320\n",
            "skipping batch 352\n",
            "training for batch 416/536\n",
            "training for batch 448/536\n",
            "training for batch 480/536\n",
            "training for batch 512/536\n",
            "epoch 1/30, batch 512/536, loss=0.5151680707931519\n",
            "saving model\n",
            "saved model\n",
            "epoch 1, training_loss = 16.733849313082313\n",
            "saving model\n",
            "saved model\n",
            "training for batch 32/536\n",
            "training for batch 64/536\n",
            "training for batch 96/536\n",
            "training for batch 128/536\n",
            "epoch 2/30, batch 128/536, loss=0.2738741636276245\n",
            "saving model\n",
            "saved model\n",
            "training for batch 160/536\n",
            "training for batch 192/536\n",
            "training for batch 224/536\n",
            "training for batch 256/536\n",
            "epoch 2/30, batch 256/536, loss=0.7467760443687439\n",
            "saving model\n",
            "saved model\n",
            "training for batch 288/536\n",
            "training for batch 320/536\n",
            "training for batch 352/536\n",
            "training for batch 384/536\n",
            "epoch 2/30, batch 384/536, loss=0.6831027269363403\n",
            "saving model\n",
            "saved model\n",
            "training for batch 416/536\n",
            "training for batch 448/536\n",
            "training for batch 480/536\n",
            "training for batch 512/536\n",
            "epoch 2/30, batch 512/536, loss=0.47045284509658813\n",
            "saving model\n",
            "saved model\n",
            "epoch 2, training_loss = 1.176675744399901\n",
            "saving model\n",
            "saved model\n",
            "training for batch 32/536\n",
            "training for batch 64/536\n",
            "training for batch 96/536\n",
            "training for batch 128/536\n",
            "epoch 3/30, batch 128/536, loss=0.46896952390670776\n",
            "saving model\n",
            "saved model\n",
            "training for batch 160/536\n",
            "training for batch 192/536\n",
            "training for batch 224/536\n",
            "training for batch 256/536\n",
            "epoch 3/30, batch 256/536, loss=0.647692084312439\n",
            "saving model\n",
            "saved model\n",
            "training for batch 288/536\n",
            "training for batch 320/536\n",
            "training for batch 352/536\n",
            "training for batch 384/536\n",
            "epoch 3/30, batch 384/536, loss=0.6166831851005554\n",
            "saving model\n",
            "saved model\n",
            "training for batch 416/536\n",
            "training for batch 448/536\n",
            "training for batch 480/536\n",
            "training for batch 512/536\n",
            "epoch 3/30, batch 512/536, loss=0.6364210247993469\n",
            "saving model\n",
            "saved model\n",
            "epoch 3, training_loss = 1.7183020138668257\n",
            "saving model\n",
            "saved model\n",
            "training for batch 32/536\n",
            "training for batch 64/536\n",
            "training for batch 96/536\n",
            "training for batch 128/536\n",
            "epoch 4/30, batch 128/536, loss=0.26829683780670166\n",
            "saving model\n",
            "saved model\n",
            "training for batch 160/536\n",
            "training for batch 192/536\n",
            "training for batch 224/536\n",
            "training for batch 256/536\n",
            "epoch 4/30, batch 256/536, loss=0.5932198762893677\n",
            "saving model\n",
            "saved model\n",
            "training for batch 288/536\n",
            "training for batch 320/536\n",
            "training for batch 352/536\n",
            "training for batch 384/536\n",
            "epoch 4/30, batch 384/536, loss=0.6661200523376465\n",
            "saving model\n",
            "saved model\n",
            "training for batch 416/536\n",
            "training for batch 448/536\n",
            "training for batch 480/536\n",
            "training for batch 512/536\n",
            "epoch 4/30, batch 512/536, loss=0.6151968240737915\n",
            "saving model\n",
            "saved model\n",
            "epoch 4, training_loss = 1.5335075130352556\n",
            "saving model\n",
            "saved model\n",
            "training for batch 32/536\n",
            "training for batch 64/536\n",
            "training for batch 96/536\n",
            "training for batch 128/536\n",
            "epoch 5/30, batch 128/536, loss=0.4841659367084503\n",
            "saving model\n",
            "saved model\n",
            "training for batch 160/536\n",
            "training for batch 192/536\n",
            "training for batch 224/536\n",
            "training for batch 256/536\n",
            "epoch 5/30, batch 256/536, loss=0.6687257289886475\n",
            "saving model\n",
            "saved model\n",
            "training for batch 288/536\n",
            "training for batch 320/536\n",
            "training for batch 352/536\n",
            "training for batch 384/536\n",
            "epoch 5/30, batch 384/536, loss=0.6356951594352722\n",
            "saving model\n",
            "saved model\n",
            "training for batch 416/536\n",
            "training for batch 448/536\n",
            "training for batch 480/536\n",
            "training for batch 512/536\n",
            "epoch 5/30, batch 512/536, loss=0.3945847153663635\n",
            "saving model\n",
            "saved model\n",
            "epoch 5, training_loss = 1.227592145329091\n",
            "saving model\n",
            "saved model\n",
            "training for batch 32/536\n",
            "training for batch 64/536\n",
            "training for batch 96/536\n",
            "training for batch 128/536\n",
            "epoch 6/30, batch 128/536, loss=0.35857921838760376\n",
            "saving model\n",
            "saved model\n",
            "training for batch 160/536\n",
            "training for batch 192/536\n",
            "training for batch 224/536\n",
            "training for batch 256/536\n",
            "epoch 6/30, batch 256/536, loss=0.8256452083587646\n",
            "saving model\n",
            "saved model\n",
            "training for batch 288/536\n",
            "training for batch 320/536\n",
            "training for batch 352/536\n",
            "training for batch 384/536\n",
            "epoch 6/30, batch 384/536, loss=0.49763521552085876\n",
            "saving model\n",
            "saved model\n",
            "training for batch 416/536\n",
            "training for batch 448/536\n",
            "training for batch 480/536\n",
            "training for batch 512/536\n",
            "epoch 6/30, batch 512/536, loss=0.3280705511569977\n",
            "saving model\n",
            "saved model\n",
            "epoch 6, training_loss = 1.0052283195250515\n",
            "saving model\n",
            "saved model\n",
            "training for batch 32/536\n",
            "training for batch 64/536\n",
            "training for batch 96/536\n",
            "training for batch 128/536\n",
            "epoch 7/30, batch 128/536, loss=0.17733284831047058\n",
            "saving model\n",
            "saved model\n",
            "training for batch 160/536\n",
            "training for batch 192/536\n",
            "training for batch 224/536\n",
            "training for batch 256/536\n",
            "epoch 7/30, batch 256/536, loss=0.5600597858428955\n",
            "saving model\n",
            "saved model\n",
            "training for batch 288/536\n",
            "training for batch 320/536\n",
            "training for batch 352/536\n",
            "training for batch 384/536\n",
            "epoch 7/30, batch 384/536, loss=0.2653951346874237\n",
            "saving model\n",
            "saved model\n",
            "training for batch 416/536\n",
            "training for batch 448/536\n",
            "training for batch 480/536\n",
            "training for batch 512/536\n",
            "epoch 7/30, batch 512/536, loss=0.4191661775112152\n",
            "saving model\n",
            "saved model\n",
            "epoch 7, training_loss = 1.0125426853687238\n",
            "saving model\n",
            "saved model\n",
            "training for batch 32/536\n",
            "training for batch 64/536\n",
            "training for batch 96/536\n",
            "training for batch 128/536\n",
            "epoch 8/30, batch 128/536, loss=0.22296251356601715\n",
            "saving model\n",
            "saved model\n",
            "training for batch 160/536\n",
            "training for batch 192/536\n",
            "training for batch 224/536\n",
            "training for batch 256/536\n",
            "epoch 8/30, batch 256/536, loss=0.6019638180732727\n",
            "saving model\n",
            "saved model\n",
            "training for batch 288/536\n",
            "training for batch 320/536\n",
            "training for batch 352/536\n",
            "training for batch 384/536\n",
            "epoch 8/30, batch 384/536, loss=0.4672914147377014\n",
            "saving model\n",
            "saved model\n",
            "training for batch 416/536\n",
            "training for batch 448/536\n",
            "training for batch 480/536\n",
            "training for batch 512/536\n",
            "epoch 8/30, batch 512/536, loss=0.3280525803565979\n",
            "saving model\n",
            "saved model\n",
            "epoch 8, training_loss = 1.272312203547757\n",
            "saving model\n",
            "saved model\n",
            "training for batch 32/536\n",
            "training for batch 64/536\n",
            "training for batch 96/536\n",
            "training for batch 128/536\n",
            "epoch 9/30, batch 128/536, loss=0.25862956047058105\n",
            "saving model\n",
            "saved model\n",
            "training for batch 160/536\n",
            "training for batch 192/536\n",
            "training for batch 224/536\n",
            "training for batch 256/536\n",
            "epoch 9/30, batch 256/536, loss=0.6069986820220947\n",
            "saving model\n",
            "saved model\n",
            "training for batch 288/536\n",
            "training for batch 320/536\n",
            "training for batch 352/536\n",
            "training for batch 384/536\n",
            "epoch 9/30, batch 384/536, loss=0.3691060245037079\n",
            "saving model\n",
            "saved model\n",
            "training for batch 416/536\n",
            "training for batch 448/536\n",
            "training for batch 480/536\n",
            "training for batch 512/536\n",
            "epoch 9/30, batch 512/536, loss=0.2953627109527588\n",
            "saving model\n",
            "saved model\n",
            "epoch 9, training_loss = 0.7788897847353634\n",
            "saving model\n",
            "saved model\n",
            "training for batch 32/536\n",
            "training for batch 64/536\n",
            "training for batch 96/536\n",
            "training for batch 128/536\n",
            "epoch 10/30, batch 128/536, loss=0.09719619154930115\n",
            "saving model\n",
            "saved model\n",
            "training for batch 160/536\n",
            "training for batch 192/536\n",
            "training for batch 224/536\n",
            "training for batch 256/536\n",
            "epoch 10/30, batch 256/536, loss=0.526077389717102\n",
            "saving model\n",
            "saved model\n",
            "training for batch 288/536\n",
            "training for batch 320/536\n",
            "training for batch 352/536\n",
            "training for batch 384/536\n",
            "epoch 10/30, batch 384/536, loss=0.6033291816711426\n",
            "saving model\n",
            "saved model\n",
            "training for batch 416/536\n",
            "training for batch 448/536\n",
            "training for batch 480/536\n",
            "training for batch 512/536\n",
            "epoch 10/30, batch 512/536, loss=0.36765581369400024\n",
            "saving model\n",
            "saved model\n",
            "epoch 10, training_loss = 0.8001139758522214\n",
            "saving model\n",
            "saved model\n",
            "training for batch 32/536\n",
            "training for batch 64/536\n",
            "training for batch 96/536\n",
            "training for batch 128/536\n",
            "epoch 11/30, batch 128/536, loss=0.2738846242427826\n",
            "saving model\n",
            "saved model\n",
            "training for batch 160/536\n",
            "training for batch 192/536\n",
            "training for batch 224/536\n",
            "training for batch 256/536\n",
            "epoch 11/30, batch 256/536, loss=0.4822259545326233\n",
            "saving model\n",
            "saved model\n",
            "training for batch 288/536\n",
            "training for batch 320/536\n",
            "training for batch 352/536\n",
            "training for batch 384/536\n",
            "epoch 11/30, batch 384/536, loss=0.4517193138599396\n",
            "saving model\n",
            "saved model\n",
            "training for batch 416/536\n",
            "training for batch 448/536\n",
            "training for batch 480/536\n",
            "training for batch 512/536\n",
            "epoch 11/30, batch 512/536, loss=0.41640037298202515\n",
            "saving model\n",
            "saved model\n",
            "epoch 11, training_loss = 0.9962688474637915\n",
            "saving model\n",
            "saved model\n",
            "training for batch 32/536\n",
            "training for batch 64/536\n",
            "training for batch 96/536\n",
            "training for batch 128/536\n",
            "epoch 12/30, batch 128/536, loss=0.26675423979759216\n",
            "saving model\n",
            "saved model\n",
            "training for batch 160/536\n",
            "training for batch 192/536\n",
            "training for batch 224/536\n",
            "training for batch 256/536\n",
            "epoch 12/30, batch 256/536, loss=0.4823877811431885\n",
            "saving model\n",
            "saved model\n",
            "training for batch 288/536\n",
            "training for batch 320/536\n",
            "training for batch 352/536\n",
            "training for batch 384/536\n",
            "epoch 12/30, batch 384/536, loss=0.3636074364185333\n",
            "saving model\n",
            "saved model\n",
            "training for batch 416/536\n",
            "training for batch 448/536\n",
            "training for batch 480/536\n",
            "training for batch 512/536\n",
            "epoch 12/30, batch 512/536, loss=0.29632067680358887\n",
            "saving model\n",
            "saved model\n",
            "epoch 12, training_loss = 0.7581010682121964\n",
            "saving model\n",
            "saved model\n",
            "training for batch 32/536\n",
            "training for batch 64/536\n",
            "training for batch 96/536\n",
            "training for batch 128/536\n",
            "epoch 13/30, batch 128/536, loss=1.2337877750396729\n",
            "saving model\n",
            "saved model\n",
            "training for batch 160/536\n",
            "training for batch 192/536\n",
            "training for batch 224/536\n",
            "training for batch 256/536\n",
            "epoch 13/30, batch 256/536, loss=0.4360533058643341\n",
            "saving model\n",
            "saved model\n",
            "training for batch 288/536\n",
            "training for batch 320/536\n",
            "training for batch 352/536\n",
            "training for batch 384/536\n",
            "epoch 13/30, batch 384/536, loss=0.44907915592193604\n",
            "saving model\n",
            "saved model\n",
            "training for batch 416/536\n",
            "training for batch 448/536\n",
            "training for batch 480/536\n",
            "training for batch 512/536\n",
            "epoch 13/30, batch 512/536, loss=0.20411691069602966\n",
            "saving model\n",
            "saved model\n",
            "epoch 13, training_loss = 0.587017597825985\n",
            "saving model\n",
            "saved model\n",
            "training for batch 32/536\n",
            "training for batch 64/536\n",
            "training for batch 96/536\n",
            "training for batch 128/536\n",
            "epoch 14/30, batch 128/536, loss=0.16136407852172852\n",
            "saving model\n",
            "saved model\n",
            "training for batch 160/536\n",
            "training for batch 192/536\n",
            "training for batch 224/536\n",
            "training for batch 256/536\n",
            "epoch 14/30, batch 256/536, loss=0.32197317481040955\n",
            "saving model\n",
            "saved model\n",
            "training for batch 288/536\n",
            "training for batch 320/536\n",
            "training for batch 352/536\n",
            "training for batch 384/536\n",
            "epoch 14/30, batch 384/536, loss=0.045474469661712646\n",
            "saving model\n",
            "saved model\n",
            "training for batch 416/536\n",
            "training for batch 448/536\n",
            "training for batch 480/536\n",
            "training for batch 512/536\n",
            "epoch 14/30, batch 512/536, loss=0.19446583092212677\n",
            "saving model\n",
            "saved model\n",
            "epoch 14, training_loss = 0.43240942721916936\n",
            "saving model\n",
            "saved model\n",
            "training for batch 32/536\n",
            "training for batch 64/536\n",
            "training for batch 96/536\n",
            "training for batch 128/536\n",
            "epoch 15/30, batch 128/536, loss=0.359863817691803\n",
            "saving model\n",
            "saved model\n",
            "training for batch 160/536\n",
            "training for batch 192/536\n",
            "training for batch 224/536\n",
            "training for batch 256/536\n",
            "epoch 15/30, batch 256/536, loss=0.09030993282794952\n",
            "saving model\n",
            "saved model\n",
            "training for batch 288/536\n",
            "training for batch 320/536\n",
            "training for batch 352/536\n",
            "training for batch 384/536\n",
            "epoch 15/30, batch 384/536, loss=0.07149761915206909\n",
            "saving model\n",
            "saved model\n",
            "training for batch 416/536\n",
            "training for batch 448/536\n",
            "training for batch 480/536\n",
            "training for batch 512/536\n",
            "epoch 15/30, batch 512/536, loss=0.15549615025520325\n",
            "saving model\n",
            "saved model\n",
            "epoch 15, training_loss = 0.5764788968279988\n",
            "saving model\n",
            "saved model\n",
            "training for batch 32/536\n",
            "training for batch 64/536\n",
            "training for batch 96/536\n",
            "training for batch 128/536\n",
            "epoch 16/30, batch 128/536, loss=0.021571695804595947\n",
            "saving model\n",
            "saved model\n",
            "training for batch 160/536\n",
            "training for batch 192/536\n",
            "training for batch 224/536\n",
            "training for batch 256/536\n",
            "epoch 16/30, batch 256/536, loss=0.03450298309326172\n",
            "saving model\n",
            "saved model\n",
            "training for batch 288/536\n",
            "training for batch 320/536\n",
            "training for batch 352/536\n",
            "training for batch 384/536\n",
            "epoch 16/30, batch 384/536, loss=0.16088342666625977\n",
            "saving model\n",
            "saved model\n",
            "training for batch 416/536\n",
            "training for batch 448/536\n",
            "training for batch 480/536\n",
            "training for batch 512/536\n",
            "epoch 16/30, batch 512/536, loss=0.07186725735664368\n",
            "saving model\n",
            "saved model\n",
            "epoch 16, training_loss = 0.2913992616639151\n",
            "saving model\n",
            "saved model\n",
            "training for batch 32/536\n",
            "training for batch 64/536\n",
            "training for batch 96/536\n",
            "training for batch 128/536\n",
            "epoch 17/30, batch 128/536, loss=0.026584267616271973\n",
            "saving model\n",
            "saved model\n",
            "training for batch 160/536\n",
            "training for batch 192/536\n",
            "training for batch 224/536\n",
            "training for batch 256/536\n",
            "epoch 17/30, batch 256/536, loss=0.0024700164794921875\n",
            "saving model\n",
            "saved model\n",
            "training for batch 288/536\n",
            "training for batch 320/536\n",
            "training for batch 352/536\n",
            "training for batch 384/536\n",
            "epoch 17/30, batch 384/536, loss=0.028525352478027344\n",
            "saving model\n",
            "saved model\n",
            "training for batch 416/536\n",
            "training for batch 448/536\n",
            "training for batch 480/536\n",
            "training for batch 512/536\n",
            "epoch 17/30, batch 512/536, loss=0.4005545675754547\n",
            "saving model\n",
            "saved model\n",
            "epoch 17, training_loss = 0.457749860270607\n",
            "saving model\n",
            "saved model\n",
            "training for batch 32/536\n",
            "training for batch 64/536\n",
            "training for batch 96/536\n",
            "training for batch 128/536\n",
            "epoch 18/30, batch 128/536, loss=1.0125902891159058\n",
            "saving model\n",
            "saved model\n",
            "training for batch 160/536\n",
            "training for batch 192/536\n",
            "training for batch 224/536\n",
            "training for batch 256/536\n",
            "epoch 18/30, batch 256/536, loss=0.20000986754894257\n",
            "saving model\n",
            "saved model\n",
            "training for batch 288/536\n",
            "training for batch 320/536\n",
            "training for batch 352/536\n",
            "training for batch 384/536\n",
            "epoch 18/30, batch 384/536, loss=0.05285397171974182\n",
            "saving model\n",
            "saved model\n",
            "training for batch 416/536\n",
            "training for batch 448/536\n",
            "training for batch 480/536\n",
            "training for batch 512/536\n",
            "epoch 18/30, batch 512/536, loss=0.5646783113479614\n",
            "saving model\n",
            "saved model\n",
            "epoch 18, training_loss = 0.970490568559477\n",
            "saving model\n",
            "saved model\n",
            "training for batch 32/536\n",
            "training for batch 64/536\n",
            "training for batch 96/536\n",
            "training for batch 128/536\n",
            "epoch 19/30, batch 128/536, loss=0.5842548608779907\n",
            "saving model\n",
            "saved model\n",
            "training for batch 160/536\n",
            "training for batch 192/536\n",
            "training for batch 224/536\n",
            "training for batch 256/536\n",
            "epoch 19/30, batch 256/536, loss=0.30721092224121094\n",
            "saving model\n",
            "saved model\n",
            "training for batch 288/536\n",
            "training for batch 320/536\n",
            "training for batch 352/536\n",
            "training for batch 384/536\n",
            "epoch 19/30, batch 384/536, loss=0.6526025533676147\n",
            "saving model\n",
            "saved model\n",
            "training for batch 416/536\n",
            "training for batch 448/536\n",
            "training for batch 480/536\n",
            "training for batch 512/536\n",
            "epoch 19/30, batch 512/536, loss=0.37290096282958984\n",
            "saving model\n",
            "saved model\n",
            "epoch 19, training_loss = 0.9936738967382189\n",
            "saving model\n",
            "saved model\n",
            "training for batch 32/536\n",
            "training for batch 64/536\n",
            "training for batch 96/536\n",
            "training for batch 128/536\n",
            "epoch 20/30, batch 128/536, loss=0.45280978083610535\n",
            "saving model\n",
            "saved model\n",
            "training for batch 160/536\n",
            "training for batch 192/536\n",
            "training for batch 224/536\n",
            "training for batch 256/536\n",
            "epoch 20/30, batch 256/536, loss=0.5559837818145752\n",
            "saving model\n",
            "saved model\n",
            "training for batch 288/536\n",
            "training for batch 320/536\n",
            "training for batch 352/536\n",
            "training for batch 384/536\n",
            "epoch 20/30, batch 384/536, loss=0.6091353297233582\n",
            "saving model\n",
            "saved model\n",
            "training for batch 416/536\n",
            "training for batch 448/536\n",
            "training for batch 480/536\n",
            "training for batch 512/536\n",
            "epoch 20/30, batch 512/536, loss=0.2583419978618622\n",
            "saving model\n",
            "saved model\n",
            "epoch 20, training_loss = 0.8636930573209529\n",
            "saving model\n",
            "saved model\n",
            "training for batch 32/536\n",
            "training for batch 64/536\n",
            "training for batch 96/536\n",
            "training for batch 128/536\n",
            "epoch 21/30, batch 128/536, loss=0.33043205738067627\n",
            "saving model\n",
            "saved model\n",
            "training for batch 160/536\n",
            "training for batch 192/536\n",
            "training for batch 224/536\n",
            "training for batch 256/536\n",
            "epoch 21/30, batch 256/536, loss=0.4631010890007019\n",
            "saving model\n",
            "saved model\n",
            "training for batch 288/536\n",
            "training for batch 320/536\n",
            "training for batch 352/536\n",
            "training for batch 384/536\n",
            "epoch 21/30, batch 384/536, loss=0.02849864959716797\n",
            "saving model\n",
            "saved model\n",
            "training for batch 416/536\n",
            "training for batch 448/536\n",
            "training for batch 480/536\n",
            "training for batch 512/536\n",
            "epoch 21/30, batch 512/536, loss=0.35299152135849\n",
            "saving model\n",
            "saved model\n",
            "epoch 21, training_loss = 0.6612988200626826\n",
            "saving model\n",
            "saved model\n",
            "training for batch 32/536\n",
            "training for batch 64/536\n",
            "training for batch 96/536\n",
            "training for batch 128/536\n",
            "epoch 22/30, batch 128/536, loss=0.17701277136802673\n",
            "saving model\n",
            "saved model\n",
            "training for batch 160/536\n",
            "training for batch 192/536\n",
            "training for batch 224/536\n",
            "training for batch 256/536\n",
            "epoch 22/30, batch 256/536, loss=0.014991581439971924\n",
            "saving model\n",
            "saved model\n",
            "training for batch 288/536\n",
            "training for batch 320/536\n",
            "training for batch 352/536\n",
            "training for batch 384/536\n",
            "epoch 22/30, batch 384/536, loss=0.5808585286140442\n",
            "saving model\n",
            "saved model\n",
            "training for batch 416/536\n",
            "training for batch 448/536\n",
            "training for batch 480/536\n",
            "training for batch 512/536\n",
            "epoch 22/30, batch 512/536, loss=0.21871621906757355\n",
            "saving model\n",
            "saved model\n",
            "epoch 22, training_loss = 0.7097590320178395\n",
            "saving model\n",
            "saved model\n",
            "training for batch 32/536\n",
            "training for batch 64/536\n",
            "training for batch 96/536\n",
            "training for batch 128/536\n",
            "epoch 23/30, batch 128/536, loss=0.29177361726760864\n",
            "saving model\n",
            "saved model\n",
            "training for batch 160/536\n",
            "training for batch 192/536\n",
            "training for batch 224/536\n",
            "training for batch 256/536\n",
            "epoch 23/30, batch 256/536, loss=0.41586369276046753\n",
            "saving model\n",
            "saved model\n",
            "training for batch 288/536\n",
            "training for batch 320/536\n",
            "training for batch 352/536\n",
            "training for batch 384/536\n",
            "epoch 23/30, batch 384/536, loss=0.6298044919967651\n",
            "saving model\n",
            "saved model\n",
            "training for batch 416/536\n",
            "training for batch 448/536\n",
            "training for batch 480/536\n",
            "training for batch 512/536\n",
            "epoch 23/30, batch 512/536, loss=0.35991808772087097\n",
            "saving model\n",
            "saved model\n",
            "epoch 23, training_loss = 0.9921677944236504\n",
            "saving model\n",
            "saved model\n",
            "training for batch 32/536\n",
            "training for batch 64/536\n",
            "training for batch 96/536\n",
            "training for batch 128/536\n",
            "epoch 24/30, batch 128/536, loss=0.336805522441864\n",
            "saving model\n",
            "saved model\n",
            "training for batch 160/536\n",
            "training for batch 192/536\n",
            "training for batch 224/536\n",
            "training for batch 256/536\n",
            "epoch 24/30, batch 256/536, loss=8.03103256225586\n",
            "saving model\n",
            "saved model\n",
            "training for batch 288/536\n",
            "training for batch 320/536\n",
            "training for batch 352/536\n",
            "training for batch 384/536\n",
            "epoch 24/30, batch 384/536, loss=0.520325243473053\n",
            "saving model\n",
            "saved model\n",
            "training for batch 416/536\n",
            "training for batch 448/536\n",
            "training for batch 480/536\n",
            "training for batch 512/536\n",
            "epoch 24/30, batch 512/536, loss=0.29927361011505127\n",
            "saving model\n",
            "saved model\n",
            "epoch 24, training_loss = 3.7959920697073963\n",
            "saving model\n",
            "saved model\n",
            "training for batch 32/536\n",
            "training for batch 64/536\n",
            "training for batch 96/536\n",
            "training for batch 128/536\n",
            "epoch 25/30, batch 128/536, loss=0.6409049034118652\n",
            "saving model\n",
            "saved model\n",
            "training for batch 160/536\n",
            "training for batch 192/536\n",
            "training for batch 224/536\n",
            "training for batch 256/536\n",
            "epoch 25/30, batch 256/536, loss=0.25754284858703613\n",
            "saving model\n",
            "saved model\n",
            "training for batch 288/536\n",
            "training for batch 320/536\n",
            "training for batch 352/536\n",
            "training for batch 384/536\n",
            "epoch 25/30, batch 384/536, loss=0.27332383394241333\n",
            "saving model\n",
            "saved model\n",
            "training for batch 416/536\n",
            "training for batch 448/536\n",
            "training for batch 480/536\n",
            "training for batch 512/536\n",
            "epoch 25/30, batch 512/536, loss=0.6436083316802979\n",
            "saving model\n",
            "saved model\n",
            "epoch 25, training_loss = 0.7990044273693225\n",
            "saving model\n",
            "saved model\n",
            "training for batch 32/536\n",
            "training for batch 64/536\n",
            "training for batch 96/536\n",
            "training for batch 128/536\n",
            "epoch 26/30, batch 128/536, loss=0.30371496081352234\n",
            "saving model\n",
            "saved model\n",
            "training for batch 160/536\n",
            "training for batch 192/536\n",
            "training for batch 224/536\n",
            "training for batch 256/536\n",
            "epoch 26/30, batch 256/536, loss=0.3867853283882141\n",
            "saving model\n",
            "saved model\n",
            "training for batch 288/536\n",
            "training for batch 320/536\n",
            "training for batch 352/536\n",
            "training for batch 384/536\n",
            "epoch 26/30, batch 384/536, loss=0.0121346116065979\n",
            "saving model\n",
            "saved model\n",
            "training for batch 416/536\n",
            "training for batch 448/536\n",
            "training for batch 480/536\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "n_epochs = 30\n",
        "n_samples = len(train_dataset)\n",
        "n_iterations = ceil(n_samples / batch_size)\n",
        "\n",
        "input_dim = (3, 90, 240, 320)\n",
        "model = ConvNet3D(input_dim)\n",
        "\n",
        "# Calculate pos_weight based on class imbalance\n",
        "pos_weight = torch.tensor([1910 / 231]) #manually counted\n",
        "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight.to(device))\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
        "#scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10)\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "# Load a previously saved checkpoint (if resuming)\n",
        "checkpoint_file = 'second_model_checkpoint.pth'\n",
        "checkpoint_dir = '/content/drive/Shareddrives/thesis/models/checkpoints/'\n",
        "checkpoint_path = checkpoint_dir + checkpoint_file\n",
        "!mkdir -p $checkpoint_dir\n",
        "start_epoch = 0\n",
        "start_batch = 0\n",
        "epoch_loss = [] #loss for each batch\n",
        "train_loss = [] #gets average of epoch_loss every epoch\n",
        "try:\n",
        "    checkpoint = torch.load(checkpoint_path)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    #scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "    start_epoch = checkpoint['epoch']\n",
        "    start_batch = checkpoint['batch']\n",
        "    train_loss = checkpoint['train_loss']\n",
        "    epoch_loss = checkpoint['epoch_loss']\n",
        "    print(f\"Resuming training from epoch {start_epoch + 1}, batch {start_batch}\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Starting training from scratch.\")\n",
        "\n",
        "# clips_without_frames = set()\n",
        "# clips_without_frames.add\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(start_epoch, n_epochs):\n",
        "  #current_lr = scheduler.get_last_lr()[0]\n",
        "  #print(f\"Current Learning Rate: {current_lr}\")\n",
        "  for i, (_, inputs, labels) in enumerate(train_dataloader):\n",
        "    if epoch == start_epoch and i < start_batch:\n",
        "      if i % 32 == 0:\n",
        "        print(f\"skipping batch {i}\")\n",
        "      continue\n",
        "    if (i + 1) % 32 == 0: print(f\"training for batch {i + 1}/{n_iterations}\")\n",
        "\n",
        "    inputs = inputs.to(device)\n",
        "    labels = labels.to(device)\n",
        "    preds = model(inputs)\n",
        "\n",
        "    # calculate loss\n",
        "    optimizer.zero_grad()\n",
        "    loss = criterion(preds, labels)\n",
        "    epoch_loss.append(loss.item())\n",
        "\n",
        "    #update weights\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Save a checkpoint every 128 batches\n",
        "    if (i + 1) % 128 == 0:\n",
        "      print(f\"epoch {epoch + 1}/{n_epochs}, batch {i + 1}/{n_iterations}, loss={loss.item()}\")\n",
        "      print(f\"saving model\")\n",
        "      torch.save({\n",
        "          'epoch': epoch,\n",
        "          'batch': i + 1,\n",
        "          'model_state_dict': model.state_dict(),\n",
        "          'optimizer_state_dict': optimizer.state_dict(),\n",
        "          'epoch_loss': epoch_loss,\n",
        "          'train_loss': train_loss,\n",
        "          # 'scheduler_state_dict': scheduler.state_dict(),\n",
        "      }, checkpoint_path)\n",
        "      print(\"saved model\")\n",
        "\n",
        "\n",
        "  # save after epoch\n",
        "  train_loss.append(sum(epoch_loss) / len(epoch_loss))\n",
        "  epoch_loss = []\n",
        "  print(f\"epoch {epoch + 1}, training_loss = {train_loss[-1]}\")\n",
        "  print(\"saving model\")\n",
        "  torch.save({\n",
        "            'epoch': epoch + 1,\n",
        "            'batch': 0,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'epoch_loss': [],\n",
        "            'train_loss': train_loss,\n",
        "            #'scheduler_state_dict': scheduler.state_dict(),\n",
        "        }, checkpoint_path)\n",
        "  # scheduler.step(loss)\n",
        "  print(\"saved model\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nv_2sZp5LRAl"
      },
      "source": [
        "## Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "n5_piCOQfpZg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb69620b-534b-4122-d82e-43ab3a2afe26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-9ea2b3900307>:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(checkpoint_path)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "testing for batch 10/134\n",
            "testing for batch 20/134\n",
            "testing for batch 30/134\n",
            "testing for batch 40/134\n",
            "testing for batch 50/134\n",
            "testing for batch 60/134\n",
            "testing for batch 70/134\n",
            "testing for batch 80/134\n",
            "testing for batch 90/134\n",
            "testing for batch 100/134\n",
            "testing for batch 110/134\n",
            "testing for batch 120/134\n",
            "testing for batch 130/134\n",
            "tested all 134 batches\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "# validation\n",
        "input_dim = (3, 90, 240, 320)\n",
        "model = ConvNet3D(input_dim)\n",
        "threshold = 0.4\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# Load model\n",
        "checkpoint_file = 'second_model_checkpoint.pth'\n",
        "checkpoint_path = '/content/drive/Shareddrives/thesis/models/checkpoints/' + checkpoint_file\n",
        "checkpoint = torch.load(checkpoint_path)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "model = model.to(device)\n",
        "\n",
        "indices_all = []\n",
        "y_true_all = []\n",
        "y_pred_all = []\n",
        "y_prob_all = []\n",
        "n_iterations = ceil(len(test_dataset) / batch_size)\n",
        "\n",
        "with torch.no_grad():\n",
        "  for batch, (indices, inputs, labels) in enumerate(test_dataloader):\n",
        "    if (batch + 1) % 10 == 0: print(f\"testing for batch {batch + 1}/{n_iterations}\")\n",
        "    inputs = inputs.to(device)\n",
        "\n",
        "    outputs = torch.sigmoid(model(inputs))\n",
        "    rounded = (outputs > threshold).int()\n",
        "\n",
        "    indices_all.extend(indices.numpy())\n",
        "    y_true_all.extend(labels.numpy())\n",
        "    y_prob_all.extend(outputs.cpu().numpy())\n",
        "    y_pred_all.extend(rounded.cpu().numpy())\n",
        "  print(f'tested all {n_iterations} batches')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "QbFoYQpeAE0r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ad3bdc7-fca4-43a6-a7de-d8340ad0d73e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8619\n",
            "Precision: 0.3621\n",
            "Recall: 0.3621\n",
            "F1 Score: 0.3621\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-14-364c59edf9ce>:35: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
            "  metrics = calculate_metrics(torch.tensor(indices_all), torch.tensor(y_pred_all), torch.tensor(y_true_all))\n"
          ]
        }
      ],
      "source": [
        "false_positives = []\n",
        "false_negatives = []\n",
        "true_positives = []\n",
        "true_negatives = []\n",
        "\n",
        "def calculate_metrics(inds, preds, labels):\n",
        "    TP = (preds == 1) & (labels == 1)\n",
        "    TN = (preds == 0) & (labels == 0)\n",
        "    FP = (preds == 1) & (labels == 0)\n",
        "    FN = (preds == 0) & (labels == 1)\n",
        "\n",
        "    true_positives.extend(inds[TP].tolist())\n",
        "    true_negatives.extend(inds[TN].tolist())\n",
        "    false_positives.extend(inds[FP].tolist())\n",
        "    false_negatives.extend(inds[FN].tolist())\n",
        "\n",
        "    TP = torch.sum(TP).item()\n",
        "    TN = torch.sum(TN).item()\n",
        "    FP = torch.sum(FP).item()\n",
        "    FN = torch.sum(FN).item()\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = (TP + TN) / (TP + TN + FP + FN) if (TP + TN + FP + FN) != 0 else 0\n",
        "    precision = TP / (TP + FP) if (TP + FP) != 0 else 0\n",
        "    recall = TP / (TP + FN) if (TP + FN) != 0 else 0\n",
        "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0\n",
        "\n",
        "    return {\n",
        "        \"accuracy\": accuracy,\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall,\n",
        "        \"f1\": f1\n",
        "    }\n",
        "\n",
        "metrics = calculate_metrics(torch.tensor(indices_all), torch.tensor(y_pred_all), torch.tensor(y_true_all))\n",
        "print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n",
        "print(f\"Precision: {metrics['precision']:.4f}\")\n",
        "print(f\"Recall: {metrics['recall']:.4f}\")\n",
        "print(f\"F1 Score: {metrics['f1']:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "PdqcARyqB0tx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8bd4a34-a3cc-4d05-c49b-8e8d5ba53e64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[tensor(0.0500), 0.3620689655172414]\n",
            "[tensor(0.0500), 0.8619402985074627]\n",
            "[tensor(0.0500), 0.3620689655172414]\n",
            "[tensor(0.0500), 0.36206896551724144]\n"
          ]
        }
      ],
      "source": [
        "precision = [0,0]\n",
        "accuracy = [0,0]\n",
        "recall = [0,0]\n",
        "f1 = [0,0]\n",
        "for threshold in torch.arange(0.05, 1, 0.05):\n",
        "  y_pred_test = (torch.tensor(y_prob_all) > threshold).int()\n",
        "  metrics = calculate_metrics(torch.tensor(indices_all), torch.tensor(y_pred_all), torch.tensor(y_true_all))\n",
        "  if precision[1] < metrics['precision']:\n",
        "    precision = [threshold, metrics['precision']]\n",
        "  if accuracy[1] < metrics['accuracy']:\n",
        "    accuracy = [threshold, metrics['accuracy']]\n",
        "  if recall[1] < metrics['recall']:\n",
        "    recall = [threshold, metrics['recall']]\n",
        "  if f1[1] < metrics['f1']:\n",
        "    f1 = [threshold, metrics['f1']]\n",
        "print(precision)\n",
        "print(accuracy)\n",
        "print(recall)\n",
        "print(f1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "3IZu6YfdqGk7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bab3296-8f54-4311-bc25-899307b4718c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.119501226 1.0\n"
          ]
        }
      ],
      "source": [
        "y_prob_all = np.array(y_prob_all)\n",
        "print(y_prob_all.mean(), y_prob_all.max())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "o3e4ScUVIXLQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "926a8084-425d-465e-df7d-e0f5938eb3b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True Positives: \n",
            "35      Shoplifting026_x264_4.mp4\n",
            "52      Shoplifting029_x264_9.mp4\n",
            "80      Shoplifting018_x264_9.mp4\n",
            "84      Shoplifting007_x264_4.mp4\n",
            "122    Shoplifting041_x264_17.mp4\n",
            "Name: Filename, dtype: object\n",
            "False Negatives: \n",
            "1      Shoplifting028_x264_4.mp4\n",
            "36    Shoplifting040_x264_16.mp4\n",
            "37     Shoplifting013_x264_7.mp4\n",
            "62     Shoplifting018_x264_3.mp4\n",
            "86    Shoplifting043_x264_51.mp4\n",
            "Name: Filename, dtype: object\n",
            "False Positives\n",
            "43     Shoplifting039_x264_21.mp4\n",
            "93     Shoplifting055_x264_47.mp4\n",
            "95     Shoplifting050_x264_17.mp4\n",
            "101    Shoplifting009_x264_34.mp4\n",
            "154    Shoplifting038_x264_10.mp4\n",
            "Name: Filename, dtype: object\n"
          ]
        }
      ],
      "source": [
        "base_path = \"/content/drive/Shareddrives/thesis/dataset/\"\n",
        "file_path = \"test.csv\"\n",
        "data = pd.read_csv(base_path + file_path)['Filename']\n",
        "false_positive_names = data.iloc[false_positives]\n",
        "false_negative_names = data.iloc[false_negatives]\n",
        "true_positive_names = data.iloc[true_positives]\n",
        "true_negative_names = data.iloc[true_negatives]\n",
        "print('True Positives: ')\n",
        "print(true_positive_names.head())\n",
        "print('False Negatives: ')\n",
        "print(false_negative_names.head())\n",
        "print('False Positives')\n",
        "print(false_positive_names.head())\n",
        "\n",
        "with open(f\"{checkpoint_path.removesuffix('.pth')}_predictions.txt\", 'w') as f:\n",
        "  f.write(\"False Positives: \\n\")\n",
        "  for item in false_positive_names:\n",
        "    f.write(f\"{item}\\n\")\n",
        "  f.write(\"\\nFalse Negatives: \\n\")\n",
        "  for item in false_negative_names:\n",
        "    f.write(f\"{item}\\n\")\n",
        "  f.write(\"\\nTrue Positives: \\n\")\n",
        "  for item in true_positive_names:\n",
        "    f.write(f\"{item}\\n\")\n",
        "  f.write(\"\\nTrue Negatives: \\n\")\n",
        "  for item in true_negative_names:\n",
        "    f.write(f\"{item}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "-M1p9xSuBPQ6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "outputId": "01fb4bf5-9d9c-4bae-f8fb-84a6a378e9a0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7a86454f8cd0>]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "training_loss = checkpoint['train_loss']\n",
        "plt.plot(training_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "IwTrbfw-3823",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "outputId": "25572384-dbc0-4d95-c2f5-f2e5c1f1c157"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Calculate the confusion matrix\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "\n",
        "cm = confusion_matrix(y_true_all, y_pred_all)\n",
        "\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "\n",
        "disp.plot()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}